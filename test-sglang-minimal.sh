#!/bin/bash

echo "ğŸ§ª SGLang ê·¹ì†Œ ë©”ëª¨ë¦¬ ì„¤ì • í…ŒìŠ¤íŠ¸"
echo "ëª©í‘œ: 3ê°œ ëª¨ë¸ì„ ë§¤ìš° ì‘ì€ ë©”ëª¨ë¦¬ë¡œ ì‹¤í–‰"
echo ""

# 1. TinyLlama - ê·¹ì†Œ ë©”ëª¨ë¦¬
echo "1ï¸âƒ£ TinyLlama ì‹œì‘ (mem-fraction: 0.05)..."
docker run -d \
  --name sglang-tiny \
  --runtime nvidia \
  --gpus all \
  -p 30001:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  --shm-size 4g \
  sglang:blackwell-final-v2 \
  --model-path TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --host 0.0.0.0 \
  --port 8000 \
  --mem-fraction-static 0.05 \
  --max-total-tokens 512 \
  --dtype float16 \
  --trust-remote-code \
  --disable-cuda-graph \
  --disable-custom-all-reduce \
  --disable-flashinfer \
  --disable-radix-cache \
  --attention-backend torch_native

echo "ëŒ€ê¸° 30ì´ˆ..."
sleep 30

echo "ë©”ëª¨ë¦¬ ìƒíƒœ:"
nvidia-smi --query-gpu=memory.used,memory.free --format=csv
docker logs sglang-tiny --tail 5

# 2. ë‘ ë²ˆì§¸ TinyLlama
echo ""
echo "2ï¸âƒ£ ë‘ ë²ˆì§¸ TinyLlama ì‹œì‘ (mem-fraction: 0.05)..."
docker run -d \
  --name sglang-tiny2 \
  --runtime nvidia \
  --gpus all \
  -p 30002:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  --shm-size 4g \
  sglang:blackwell-final-v2 \
  --model-path TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --host 0.0.0.0 \
  --port 8000 \
  --mem-fraction-static 0.05 \
  --max-total-tokens 512 \
  --dtype float16 \
  --trust-remote-code \
  --disable-cuda-graph \
  --disable-custom-all-reduce \
  --disable-flashinfer \
  --disable-radix-cache \
  --attention-backend torch_native

echo "ëŒ€ê¸° 30ì´ˆ..."
sleep 30

echo "ë©”ëª¨ë¦¬ ìƒíƒœ:"
nvidia-smi --query-gpu=memory.used,memory.free --format=csv

echo ""
echo "ì»¨í…Œì´ë„ˆ ìƒíƒœ:"
docker ps | grep sglang

echo ""
echo "ë‘ ë²ˆì§¸ ì»¨í…Œì´ë„ˆ ë¡œê·¸:"
docker logs sglang-tiny2 --tail 10
