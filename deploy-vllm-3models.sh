#!/bin/bash

echo "ğŸš€ vLLM 3ê°œ ëª¨ë¸ ë°°í¬ ì‹œì‘"
echo ""

# ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬
echo "ğŸ§¹ ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬..."
docker stop vllm-tinyllama vllm-qwen vllm-yi 2>/dev/null
docker rm vllm-tinyllama vllm-qwen vllm-yi 2>/dev/null

# GPU ìƒíƒœ í™•ì¸
echo "ğŸ“Š ì´ˆê¸° GPU ìƒíƒœ:"
nvidia-smi --query-gpu=memory.used,memory.free,memory.total --format=csv

# 1. TinyLlama ë°°í¬
echo ""
echo "1ï¸âƒ£ TinyLlama 1.1B ë°°í¬..."
docker run -d \
  --name vllm-tinyllama \
  --runtime nvidia \
  --gpus all \
  -p 40001:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  --shm-size 16g \
  vllm/vllm-openai:latest \
  --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --host 0.0.0.0 \
  --port 8000 \
  --gpu-memory-utilization 0.15 \
  --max-model-len 1024 \
  --enforce-eager

echo "â³ TinyLlama ì´ˆê¸°í™” ì¤‘ (30ì´ˆ)..."
sleep 30

# 2. Qwen 2.5-3B ë°°í¬
echo ""
echo "2ï¸âƒ£ Qwen 2.5-3B ë°°í¬..."
docker run -d \
  --name vllm-qwen \
  --runtime nvidia \
  --gpus all \
  -p 40002:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  --shm-size 24g \
  vllm/vllm-openai:latest \
  --model Qwen/Qwen2.5-3B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --gpu-memory-utilization 0.25 \
  --max-model-len 1536 \
  --enforce-eager

echo "â³ Qwen ì´ˆê¸°í™” ì¤‘ (40ì´ˆ)..."
sleep 40

# 3. Yi-6B ë°°í¬
echo ""
echo "3ï¸âƒ£ Yi-6B ë°°í¬..."
docker run -d \
  --name vllm-yi \
  --runtime nvidia \
  --gpus all \
  -p 40003:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  --shm-size 32g \
  vllm/vllm-openai:latest \
  --model 01-ai/Yi-6B-Chat \
  --host 0.0.0.0 \
  --port 8000 \
  --gpu-memory-utilization 0.45 \
  --max-model-len 2048 \
  --enforce-eager

echo "â³ Yi ì´ˆê¸°í™” ì¤‘ (50ì´ˆ)..."
sleep 50

# ìµœì¢… ìƒíƒœ í™•ì¸
echo ""
echo "âœ… vLLM ë°°í¬ ì™„ë£Œ!"
echo ""
echo "ğŸ“Š ìµœì¢… GPU ë©”ëª¨ë¦¬ ìƒíƒœ:"
nvidia-smi --query-gpu=memory.used,memory.free,memory.total --format=csv

echo ""
echo "ğŸ³ ì‹¤í–‰ ì¤‘ì¸ ì»¨í…Œì´ë„ˆ:"
docker ps | grep vllm

echo ""
echo "ğŸ“¡ API ì—”ë“œí¬ì¸íŠ¸:"
echo "  - TinyLlama: http://localhost:40001/v1/completions"
echo "  - Qwen 2.5-3B: http://localhost:40002/v1/completions"
echo "  - Yi-6B: http://localhost:40003/v1/completions"

echo ""
echo "ğŸ’¡ ë©”ëª¨ë¦¬ í• ë‹¹ ì „ëµ (vLLM):"
echo "  - TinyLlama: 15% utilization"
echo "  - Qwen: 25% utilization"
echo "  - Yi: 45% utilization"
echo "  - ì´ ì‚¬ìš©: 85% of available GPU memory"