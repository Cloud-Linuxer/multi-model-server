#!/usr/bin/env python3
"""
vLLM only benchmark with same conditions as SGLang
"""

import time
import json
import requests
import csv
from datetime import datetime
import pandas as pd
import numpy as np

# Îã§Íµ≠Ïñ¥ ÌîÑÎ°¨ÌîÑÌä∏ - SGLangÍ≥º ÎèôÏùº
PROMPTS = {
    "english": "Sing a beautiful song about spring with blooming flowers.",
    "chinese": "Âî±‰∏ÄÈ¶ñÂÖ≥‰∫éÊò•Â§©ÁôæËä±ÁõõÂºÄ‰πãÁæéÁöÑÊ≠åÊõ≤„ÄÇ",
    "korean": "Î¥ÑÏóê ÌîºÏñ¥ÎÇòÎäî ÍΩÉÎì§Ïùò ÏïÑÎ¶ÑÎã§ÏõÄÏùÑ ÎÖ∏ÎûòÌï¥Ï£ºÏÑ∏Ïöî."
}

NUM_ITERATIONS = 100  # SGLangÍ≥º ÎèôÏùºÌïú ÌÖåÏä§Ìä∏ ÌöüÏàò

def test_vllm(prompt, language, iteration):
    """vLLM ÌÖåÏä§Ìä∏"""
    url = "http://localhost:40001/v1/completions"
    payload = {
        "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        "prompt": prompt,
        "max_tokens": 100,
        "temperature": 0.7,
        "top_p": 0.9
    }

    start_time = time.perf_counter()
    try:
        response = requests.post(url, json=payload, timeout=30)
        response.raise_for_status()
        end_time = time.perf_counter()

        total_time = (end_time - start_time) * 1000  # ms

        result = response.json()
        tokens = result["usage"]["completion_tokens"]

        # TTFT Í∑ºÏÇ¨Ïπò (Ï†ÑÏ≤¥ ÏãúÍ∞ÑÏùò ÏùºÎ∂ÄÎ°ú Ï∂îÏ†ï)
        ttft_estimate = total_time * 0.1  # Ï≤´ ÌÜ†ÌÅ∞ÍπåÏßÄ ÏïΩ 10%

        return {
            "framework": "vLLM",
            "language": language,
            "iteration": iteration,
            "success": True,
            "latency_ms": total_time,
            "ttft_ms": ttft_estimate,
            "tokens": tokens,
            "tokens_per_second": tokens / (total_time / 1000) if total_time > 0 else 0,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "framework": "vLLM",
            "language": language,
            "iteration": iteration,
            "success": False,
            "error": str(e)[:200],
            "timestamp": datetime.now().isoformat()
        }

def main():
    print("=" * 70)
    print("üöÄ vLLM Multilingual Benchmark")
    print(f"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üñ•Ô∏è  GPU: NVIDIA RTX 5090 (32GB)")
    print(f"üìù Languages: English, Chinese, Korean")
    print(f"üî¢ Iterations per language: {NUM_ITERATIONS}")
    print(f"üìä Total tests: {NUM_ITERATIONS * 3}")
    print("=" * 70)

    # GPU Î©îÎ™®Î¶¨ ÌôïÏù∏
    import subprocess
    result = subprocess.run(
        ["nvidia-smi", "--query-gpu=memory.used,memory.free", "--format=csv,noheader,nounits"],
        capture_output=True, text=True
    )
    if result.returncode == 0:
        used, free = map(int, result.stdout.strip().split(", "))
        print(f"\nüìä GPU Memory: {used}MB used, {free}MB free")

    results = []

    print("\nüìù Testing vLLM...")

    for language, prompt in PROMPTS.items():
        print(f"\n  Testing {language}: ", end="", flush=True)
        success_count = 0

        for i in range(NUM_ITERATIONS):
            result = test_vllm(prompt, language, i)
            results.append(result)

            if result["success"]:
                success_count += 1

            if (i + 1) % 10 == 0:
                print(".", end="", flush=True)

            time.sleep(0.05)  # ÏöîÏ≤≠ Í∞Ñ Í∞ÑÍ≤©

        print(f" [{success_count}/{NUM_ITERATIONS} success]")

    # Í≤∞Í≥º Ï†ÄÏû•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_file = f"/home/multi-model-server/vllm_benchmark_{timestamp}.csv"

    df = pd.DataFrame(results)
    df.to_csv(csv_file, index=False)
    print(f"\nüíæ Results saved to {csv_file}")

    # Î∂ÑÏÑù
    print("\n" + "=" * 70)
    print("üìä vLLM PERFORMANCE ANALYSIS")
    print("=" * 70)

    success_df = df[df['success'] == True]

    print(f"\nüìà Overall Statistics:")
    print(f"  Total tests: {len(df)}")
    print(f"  Successful: {len(success_df)}")
    print(f"  Success rate: {len(success_df)/len(df)*100:.1f}%")

    if len(success_df) > 0:
        print(f"\n‚ö° Latency Analysis:")
        print(f"  Mean: {success_df['latency_ms'].mean():.2f}ms")
        print(f"  Median (P50): {success_df['latency_ms'].median():.2f}ms")
        print(f"  P75: {success_df['latency_ms'].quantile(0.75):.2f}ms")
        print(f"  P90: {success_df['latency_ms'].quantile(0.90):.2f}ms")
        print(f"  P95: {success_df['latency_ms'].quantile(0.95):.2f}ms")
        print(f"  P99: {success_df['latency_ms'].quantile(0.99):.2f}ms")
        print(f"  Min: {success_df['latency_ms'].min():.2f}ms")
        print(f"  Max: {success_df['latency_ms'].max():.2f}ms")
        print(f"  Std Dev: {success_df['latency_ms'].std():.2f}ms")

        print(f"\nüöÄ Throughput Analysis:")
        print(f"  Mean: {success_df['tokens_per_second'].mean():.2f} tok/s")
        print(f"  Max: {success_df['tokens_per_second'].max():.2f} tok/s")
        print(f"  Min: {success_df['tokens_per_second'].min():.2f} tok/s")

        print(f"\nüåç Language-Specific Performance:")
        for lang in ["english", "chinese", "korean"]:
            lang_df = success_df[success_df['language'] == lang]
            if len(lang_df) > 0:
                print(f"\n  {lang.capitalize()}:")
                print(f"    Success rate: {len(lang_df)}/{NUM_ITERATIONS} ({len(lang_df)/NUM_ITERATIONS*100:.1f}%)")
                print(f"    Avg latency: {lang_df['latency_ms'].mean():.2f}ms")
                print(f"    P50 latency: {lang_df['latency_ms'].median():.2f}ms")
                print(f"    P95 latency: {lang_df['latency_ms'].quantile(0.95):.2f}ms")
                print(f"    Avg throughput: {lang_df['tokens_per_second'].mean():.2f} tok/s")
                print(f"    Avg tokens: {lang_df['tokens'].mean():.1f}")

    # SGLang ÎπÑÍµê (Ïù¥Ï†Ñ Í≤∞Í≥º Ï∞∏Ï°∞)
    print("\n" + "=" * 70)
    print("üìä COMPARISON WITH SGLANG")
    print("=" * 70)

    print("\nüìù Previous SGLang Results (Reference):")
    print("  Success rate: 100% (300/300)")
    print("  Avg latency: 393.94ms")
    print("  P50 latency: 526.71ms")
    print("  P95 latency: 538.42ms")
    print("  Avg throughput: 69.38 tok/s")

    if len(success_df) > 0:
        print("\nüèÜ vLLM Current Results:")
        print(f"  Success rate: {len(success_df)/len(df)*100:.1f}%")
        print(f"  Avg latency: {success_df['latency_ms'].mean():.2f}ms")
        print(f"  P50 latency: {success_df['latency_ms'].median():.2f}ms")
        print(f"  P95 latency: {success_df['latency_ms'].quantile(0.95):.2f}ms")
        print(f"  Avg throughput: {success_df['tokens_per_second'].mean():.2f} tok/s")

        # ÎπÑÍµê Î∂ÑÏÑù
        vllm_lat = success_df['latency_ms'].mean()
        sglang_lat = 393.94
        vllm_thr = success_df['tokens_per_second'].mean()
        sglang_thr = 69.38

        print("\n‚öñÔ∏è  Performance Comparison:")
        if vllm_lat < sglang_lat:
            improvement = ((sglang_lat - vllm_lat) / sglang_lat) * 100
            print(f"  Latency: vLLM is {improvement:.1f}% faster")
        else:
            improvement = ((vllm_lat - sglang_lat) / sglang_lat) * 100
            print(f"  Latency: vLLM is {improvement:.1f}% slower")

        if vllm_thr > sglang_thr:
            improvement = ((vllm_thr - sglang_thr) / sglang_thr) * 100
            print(f"  Throughput: vLLM is {improvement:.1f}% higher")
        else:
            improvement = ((sglang_thr - vllm_thr) / sglang_thr) * 100
            print(f"  Throughput: vLLM is {improvement:.1f}% lower")

    print("\n" + "=" * 70)
    print("‚úÖ Benchmark completed!")
    print("=" * 70)

if __name__ == "__main__":
    main()