#!/bin/bash

echo "ğŸš€ SGLang 3ê°œ ëª¨ë¸ ì¡°ì •ëœ ë©”ëª¨ë¦¬ ë°°í¬"
echo "ì „ëµ: ê° ëª¨ë¸ í¬ê¸°ë³´ë‹¤ ì¶©ë¶„íˆ í° ë©”ëª¨ë¦¬ í• ë‹¹"
echo ""

# ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬
docker stop sglang-tinyllama sglang-qwen sglang-yi 2>/dev/null
docker rm sglang-tinyllama sglang-qwen sglang-yi 2>/dev/null

echo "ğŸ“Š ì´ˆê¸° GPU ìƒíƒœ:"
nvidia-smi --query-gpu=memory.used,memory.free --format=csv

# 1. TinyLlama (2.15GB ëª¨ë¸ â†’ 10% = 3.2GB)
echo ""
echo "1ï¸âƒ£ TinyLlama ë°°í¬ (mem: 0.10)..."
docker run -d \
  --name sglang-tinyllama \
  --runtime nvidia \
  --gpus all \
  -p 30001:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  -e PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True" \
  --shm-size 8g \
  sglang:blackwell-final-v2 \
  --model-path TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --host 0.0.0.0 \
  --port 8000 \
  --mem-fraction-static 0.10 \
  --max-total-tokens 1024 \
  --dtype float16 \
  --trust-remote-code \
  --disable-cuda-graph \
  --disable-custom-all-reduce \
  --disable-flashinfer \
  --disable-radix-cache \
  --attention-backend torch_native

sleep 40
echo "TinyLlama ìƒíƒœ:"
docker ps | grep sglang-tinyllama || echo "âŒ TinyLlama ì‹¤íŒ¨"
nvidia-smi --query-gpu=memory.used --format=csv

# 2. Qwen (5.88GB ëª¨ë¸ â†’ ìµœì†Œ 20% = 6.5GB)
echo ""
echo "2ï¸âƒ£ Qwen ë°°í¬ (mem: 0.20)..."
docker run -d \
  --name sglang-qwen \
  --runtime nvidia \
  --gpus all \
  -p 30002:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  -e PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True" \
  --shm-size 16g \
  sglang:blackwell-final-v2 \
  --model-path Qwen/Qwen2.5-3B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --mem-fraction-static 0.20 \
  --max-total-tokens 1024 \
  --dtype float16 \
  --trust-remote-code \
  --disable-cuda-graph \
  --disable-custom-all-reduce \
  --disable-flashinfer \
  --disable-radix-cache \
  --attention-backend torch_native

sleep 50
echo "Qwen ìƒíƒœ:"
docker ps | grep sglang-qwen || echo "âŒ Qwen ì‹¤íŒ¨"
nvidia-smi --query-gpu=memory.used --format=csv

# 3. Yi (12GB ëª¨ë¸ â†’ ìµœì†Œ 40% = 13GB)
echo ""
echo "3ï¸âƒ£ Yi ë°°í¬ (mem: 0.40)..."
docker run -d \
  --name sglang-yi \
  --runtime nvidia \
  --gpus all \
  -p 30003:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  -e PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True" \
  --shm-size 24g \
  sglang:blackwell-final-v2 \
  --model-path 01-ai/Yi-6B-Chat \
  --host 0.0.0.0 \
  --port 8000 \
  --mem-fraction-static 0.40 \
  --max-total-tokens 1024 \
  --dtype float16 \
  --trust-remote-code \
  --disable-cuda-graph \
  --disable-custom-all-reduce \
  --disable-flashinfer \
  --disable-radix-cache \
  --attention-backend torch_native

sleep 60
echo "Yi ìƒíƒœ:"
docker ps | grep sglang-yi || echo "âŒ Yi ì‹¤íŒ¨"

echo ""
echo "ğŸ“Š ìµœì¢… ìƒíƒœ:"
docker ps | grep sglang
nvidia-smi --query-gpu=memory.used,memory.free --format=csv

echo ""
echo "ğŸ“ ë©”ëª¨ë¦¬ í• ë‹¹:"
echo "  TinyLlama: 10% (3.2GB)"
echo "  Qwen: 20% (6.5GB)"
echo "  Yi: 40% (13GB)"
echo "  ì´í•©: 70% (22.7GB/32GB)"