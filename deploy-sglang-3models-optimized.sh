#!/bin/bash

echo "ğŸš€ SGLang 3ê°œ ëª¨ë¸ ìµœì í™” ë°°í¬"
echo "ë©”ëª¨ë¦¬ í• ë‹¹ ì „ëµ: ëª¨ë¸ í¬ê¸° ê¸°ë°˜ ì ì ˆí•œ í• ë‹¹"
echo ""

# ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬
echo "ğŸ§¹ ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬..."
docker stop sglang-tinyllama sglang-qwen sglang-yi 2>/dev/null
docker rm sglang-tinyllama sglang-qwen sglang-yi 2>/dev/null

# GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
echo "ğŸ“Š ì´ˆê¸° GPU ìƒíƒœ:"
nvidia-smi --query-gpu=memory.used,memory.free,memory.total --format=csv

# 1. TinyLlama (1.1B) - ê°€ì¥ ì‘ì€ ë©”ëª¨ë¦¬
echo ""
echo "1ï¸âƒ£ TinyLlama 1.1B ë°°í¬ ì¤‘..."
docker run -d \
  --name sglang-tinyllama \
  --runtime nvidia \
  --gpus all \
  -p 30001:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  -e PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True" \
  --shm-size 8g \
  sglang:blackwell-final-v2 \
  --model-path TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --host 0.0.0.0 \
  --port 8000 \
  --mem-fraction-static 0.10 \
  --max-total-tokens 2048 \
  --max-running-requests 32 \
  --dtype float16 \
  --trust-remote-code \
  --disable-cuda-graph \
  --disable-custom-all-reduce \
  --disable-flashinfer \
  --disable-radix-cache \
  --attention-backend torch_native \
  --log-level info

echo "â³ TinyLlama ì´ˆê¸°í™” ì¤‘ (30ì´ˆ)..."
sleep 30

# ë©”ëª¨ë¦¬ í™•ì¸
echo "ğŸ“Š TinyLlama ì‹¤í–‰ í›„:"
nvidia-smi --query-gpu=memory.used,memory.free --format=csv

# 2. Qwen 2.5-3B
echo ""
echo "2ï¸âƒ£ Qwen 2.5-3B ë°°í¬ ì¤‘..."
docker run -d \
  --name sglang-qwen \
  --runtime nvidia \
  --gpus all \
  -p 30002:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  -e PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True" \
  --shm-size 16g \
  sglang:blackwell-final-v2 \
  --model-path Qwen/Qwen2.5-3B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --mem-fraction-static 0.15 \
  --max-total-tokens 2048 \
  --max-running-requests 24 \
  --dtype float16 \
  --trust-remote-code \
  --disable-cuda-graph \
  --disable-custom-all-reduce \
  --disable-flashinfer \
  --disable-radix-cache \
  --attention-backend torch_native \
  --log-level info

echo "â³ Qwen ì´ˆê¸°í™” ì¤‘ (40ì´ˆ)..."
sleep 40

# ë©”ëª¨ë¦¬ í™•ì¸
echo "ğŸ“Š Qwen ì‹¤í–‰ í›„:"
nvidia-smi --query-gpu=memory.used,memory.free --format=csv

# 3. Yi-6B
echo ""
echo "3ï¸âƒ£ Yi-6B ë°°í¬ ì¤‘..."
docker run -d \
  --name sglang-yi \
  --runtime nvidia \
  --gpus all \
  -p 30003:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  -e PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True" \
  --shm-size 24g \
  sglang:blackwell-final-v2 \
  --model-path 01-ai/Yi-6B-Chat \
  --host 0.0.0.0 \
  --port 8000 \
  --mem-fraction-static 0.30 \
  --max-total-tokens 2048 \
  --max-running-requests 16 \
  --dtype float16 \
  --trust-remote-code \
  --disable-cuda-graph \
  --disable-custom-all-reduce \
  --disable-flashinfer \
  --disable-radix-cache \
  --attention-backend torch_native \
  --log-level info

echo "â³ Yi ì´ˆê¸°í™” ì¤‘ (50ì´ˆ)..."
sleep 50

# ìµœì¢… ìƒíƒœ í™•ì¸
echo ""
echo "âœ… ë°°í¬ ì™„ë£Œ!"
echo ""
echo "ğŸ“Š ìµœì¢… GPU ë©”ëª¨ë¦¬ ìƒíƒœ:"
nvidia-smi --query-gpu=memory.used,memory.free,memory.total --format=csv

echo ""
echo "ğŸ³ ì‹¤í–‰ ì¤‘ì¸ ì»¨í…Œì´ë„ˆ:"
docker ps | grep sglang

echo ""
echo "ğŸ” í—¬ìŠ¤ ì²´í¬:"
for port in 30001 30002 30003; do
  echo -n "í¬íŠ¸ $port: "
  curl -s http://localhost:$port/health 2>/dev/null && echo " âœ…" || echo " â³ ì¤€ë¹„ ì¤‘..."
done

echo ""
echo "ğŸ“¡ API ì—”ë“œí¬ì¸íŠ¸:"
echo "  - TinyLlama: http://localhost:30001/generate"
echo "  - Qwen 2.5-3B: http://localhost:30002/generate"
echo "  - Yi-6B: http://localhost:30003/generate"

echo ""
echo "ğŸ’¡ ë©”ëª¨ë¦¬ í• ë‹¹ ì „ëµ:"
echo "  - TinyLlama: 10% (3.2GB) - ëª¨ë¸ 2.15GB + KVìºì‹œ"
echo "  - Qwen: 15% (4.9GB) - ëª¨ë¸ 3GB + KVìºì‹œ"
echo "  - Yi: 30% (9.8GB) - ëª¨ë¸ 6GB + KVìºì‹œ"
echo "  - ì´ ì‚¬ìš©: 55% (ì•½ 18GB/32GB)"