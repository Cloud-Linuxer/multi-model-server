#!/bin/bash

echo "ðŸŽ¯ vLLM ì •í™•í•œ ë©”ëª¨ë¦¬ ë§¤ì¹­ í…ŒìŠ¤íŠ¸"
echo "ëª©í‘œ: SGLangê³¼ ì •í™•ížˆ ê°™ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ì•½ 2.9GB) ë‹¬ì„±"
echo ""

# ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬
docker stop $(docker ps -q) 2>/dev/null
docker rm $(docker ps -aq) 2>/dev/null
sleep 5

echo "ðŸ“Š ì´ˆê¸° GPU ìƒíƒœ:"
nvidia-smi --query-gpu=memory.used,memory.free --format=csv

# vLLMì„ ë” ë‚®ì€ ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
echo ""
echo "í…ŒìŠ¤íŠ¸ 1: vLLM gpu-memory-utilization=0.05 (ê°€ìž¥ ë‚®ì€ ì„¤ì •)"
docker run -d \
  --name vllm-5pct \
  --runtime nvidia \
  --gpus all \
  -p 40001:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  --shm-size 8g \
  vllm/vllm-openai:latest \
  --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --host 0.0.0.0 \
  --port 8000 \
  --gpu-memory-utilization 0.05 \
  --max-model-len 1024 \
  --enforce-eager

sleep 30
echo "vLLM (5%) ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:"
nvidia-smi --query-gpu=memory.used --format=csv
MEM_5PCT=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits)

docker stop vllm-5pct 2>/dev/null
docker rm vllm-5pct 2>/dev/null
sleep 5

echo ""
echo "í…ŒìŠ¤íŠ¸ 2: vLLM gpu-memory-utilization=0.03 (ë” ë‚®ì€ ì„¤ì •)"
docker run -d \
  --name vllm-3pct \
  --runtime nvidia \
  --gpus all \
  -p 40001:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  --shm-size 8g \
  vllm/vllm-openai:latest \
  --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --host 0.0.0.0 \
  --port 8000 \
  --gpu-memory-utilization 0.03 \
  --max-model-len 512 \
  --enforce-eager

sleep 30
echo "vLLM (3%) ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:"
nvidia-smi --query-gpu=memory.used --format=csv
MEM_3PCT=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits)

docker stop vllm-3pct 2>/dev/null
docker rm vllm-3pct 2>/dev/null
sleep 5

echo ""
echo "í…ŒìŠ¤íŠ¸ 3: vLLM ìµœì†Œ ì„¤ì • (gpu-util=0.01, max-len=256)"
docker run -d \
  --name vllm-minimal \
  --runtime nvidia \
  --gpus all \
  -p 40001:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  --shm-size 8g \
  vllm/vllm-openai:latest \
  --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --host 0.0.0.0 \
  --port 8000 \
  --gpu-memory-utilization 0.01 \
  --max-model-len 256 \
  --enforce-eager

sleep 30
echo "vLLM (ìµœì†Œ) ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:"
nvidia-smi --query-gpu=memory.used --format=csv
MEM_MIN=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits)

# SGLangê³¼ ë¹„êµ
echo ""
echo "í…ŒìŠ¤íŠ¸ 4: SGLang ì°¸ì¡° (mem-fraction-static=0.15)"
docker stop vllm-minimal 2>/dev/null
docker rm vllm-minimal 2>/dev/null
sleep 5

docker run -d \
  --name sglang-ref \
  --runtime nvidia \
  --gpus all \
  -p 30001:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  -e PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True" \
  --shm-size 8g \
  sglang:blackwell-final-v2 \
  --model-path TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --host 0.0.0.0 \
  --port 8000 \
  --mem-fraction-static 0.15 \
  --max-total-tokens 2048 \
  --dtype float16 \
  --trust-remote-code \
  --disable-cuda-graph \
  --disable-custom-all-reduce \
  --disable-flashinfer \
  --disable-radix-cache \
  --attention-backend torch_native

sleep 40
echo "SGLang (15%) ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:"
nvidia-smi --query-gpu=memory.used --format=csv
MEM_SGLANG=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits)

echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ðŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ ê²°ê³¼"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "SGLang (mem-fraction-static=0.15): ${MEM_SGLANG}MB (~2.9GB)"
echo ""
echo "vLLM í…ŒìŠ¤íŠ¸ ê²°ê³¼:"
echo "- gpu-memory-utilization=0.05: ${MEM_5PCT}MB"
echo "- gpu-memory-utilization=0.03: ${MEM_3PCT}MB"
echo "- gpu-memory-utilization=0.01: ${MEM_MIN}MB (ìµœì†Œ ì„¤ì •)"
echo ""
echo "ðŸ’¡ ê²°ë¡ :"
echo "- vLLMì˜ ìµœì†Œ ë©”ëª¨ë¦¬ëŠ” ëª¨ë¸ ìžì²´ í¬ê¸° + ìµœì†Œ KV ìºì‹œ"
echo "- SGLangì€ mem-fraction-staticìœ¼ë¡œ ë” ì •ë°€í•œ ì œì–´ ê°€ëŠ¥"
echo "- vLLMì€ gpu-memory-utilizationì„ 0.01ê¹Œì§€ ë‚®ì¶œ ìˆ˜ ìžˆìŒ"
echo ""

# ì—¬ëŸ¬ vLLM ì»¨í…Œì´ë„ˆ ë™ì‹œ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ðŸš€ ë™ì¼ ë©”ëª¨ë¦¬ë¡œ ì—¬ëŸ¬ ëª¨ë¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

docker stop sglang-ref 2>/dev/null
docker rm sglang-ref 2>/dev/null
sleep 5

echo "3ê°œì˜ vLLM ì»¨í…Œì´ë„ˆë¥¼ ê°ê° gpu-util=0.01ë¡œ ì‹¤í–‰..."
for i in 1 2 3; do
  docker run -d \
    --name vllm-multi-$i \
    --runtime nvidia \
    --gpus all \
    -p 4000$i:8000 \
    -v /root/.cache/huggingface:/root/.cache/huggingface \
    --shm-size 8g \
    vllm/vllm-openai:latest \
    --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
    --host 0.0.0.0 \
    --port 8000 \
    --gpu-memory-utilization 0.01 \
    --max-model-len 256 \
    --enforce-eager
  echo "vLLM ì»¨í…Œì´ë„ˆ $i ì‹œìž‘..."
  sleep 20
done

echo ""
echo "ì‹¤í–‰ ì¤‘ì¸ vLLM ì»¨í…Œì´ë„ˆ:"
docker ps | grep vllm-multi
echo ""
echo "ì´ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:"
nvidia-smi --query-gpu=memory.used --format=csv

# ì •ë¦¬
docker stop $(docker ps -q) 2>/dev/null