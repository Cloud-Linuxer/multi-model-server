# ğŸš€ Multi-Model LLM Serving on RTX 5090 - Complete Benchmarking Suite
# ğŸš€ RTX 5090ì—ì„œ ë©€í‹°ëª¨ë¸ LLM ì„œë¹™ - ì™„ë²½í•œ ë²¤ì¹˜ë§ˆí‚¹ ìŠ¤ìœ„íŠ¸

[![Performance](https://img.shields.io/badge/Throughput-332%20tok%2Fs-green)](FINAL_BENCHMARK_REPORT.md)
[![Models](https://img.shields.io/badge/Models-3%20Concurrent-blue)](MULTIMODEL_SERVING_COMPARISON.md)
[![GPU](https://img.shields.io/badge/GPU-RTX%205090%2032GB-orange)](README.md)
[![Frameworks](https://img.shields.io/badge/Frameworks-vLLM%20%7C%20SGLang%20%7C%20Ollama-purple)](FINAL_PROJECT_REPORT.md)

## ğŸ“‹ Overview / ê°œìš”

### English
This repository contains a comprehensive benchmarking suite comparing three major LLM serving frameworks (vLLM, SGLang, Ollama) for multi-model deployment on NVIDIA RTX 5090. With 4,984+ benchmark data points across multiple languages, this is the definitive guide for deploying multiple LLMs on a single GPU.

### í•œêµ­ì–´
ì´ ì €ì¥ì†ŒëŠ” NVIDIA RTX 5090ì—ì„œ ë©€í‹°ëª¨ë¸ ë°°í¬ë¥¼ ìœ„í•œ ì„¸ ê°€ì§€ ì£¼ìš” LLM ì„œë¹™ í”„ë ˆì„ì›Œí¬(vLLM, SGLang, Ollama)ë¥¼ ë¹„êµí•˜ëŠ” í¬ê´„ì ì¸ ë²¤ì¹˜ë§ˆí‚¹ ìŠ¤ìœ„íŠ¸ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ì—¬ëŸ¬ ì–¸ì–´ì— ê±¸ì¹œ 4,984ê°œ ì´ìƒì˜ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° í¬ì¸íŠ¸ë¡œ, ë‹¨ì¼ GPUì—ì„œ ì—¬ëŸ¬ LLMì„ ë°°í¬í•˜ê¸° ìœ„í•œ ê²°ì •ì ì¸ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“Š Benchmark Results Summary / ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½

| Metric / ì§€í‘œ | Ollama | vLLM | SGLang |
|--------------|--------|------|--------|
| **Latency / ì§€ì—°ì‹œê°„** | 152-348ms | 207ms | 357ms |
| **Throughput / ì²˜ë¦¬ëŸ‰*** | 64-128 tok/s | 13-112 tok/sâ€  | 77 tok/s |
| **Memory (Single) / ë©”ëª¨ë¦¬ (ë‹¨ì¼)** | ~3GB | ~9GB | ~5GB |
| **Memory (Multi) / ë©”ëª¨ë¦¬ (ë©€í‹°)** | 8.5GB (3 models) | 27GB (3 models) | N/A |
| **Multi-model / ë©€í‹°ëª¨ë¸** | âœ… Dynamic | âœ… Static | âš ï¸ Limited |
| **RTX 5090 Support** | âœ… Native | âœ… Native | âš ï¸ Partial |

*Throughput varies significantly based on token counting method (word vs subword)
*ì²˜ë¦¬ëŸ‰ì€ í† í° ê³„ì‚° ë°©ì‹(ë‹¨ì–´ vs ì„œë¸Œì›Œë“œ)ì— ë”°ë¼ í¬ê²Œ ë‹¬ë¼ì§
â€ Lower values from word-based counting, higher from token-based

**Note**: Direct comparison is limited due to different test methodologies. See [detailed report](OBJECTIVE_BENCHMARK_REPORT.md) for context.

### ğŸš€ Models Tested / í…ŒìŠ¤íŠ¸ëœ ëª¨ë¸
- **TinyLlama 1.1B**: Fast responses for simple queries / ê°„ë‹¨í•œ ì¿¼ë¦¬ë¥¼ ìœ„í•œ ë¹ ë¥¸ ì‘ë‹µ
- **Qwen 2.5-3B**: General-purpose tasks and code generation / ë²”ìš© ì‘ì—… ë° ì½”ë“œ ìƒì„±
- **Yi-6B**: High-quality responses for complex tasks / ë³µì¡í•œ ì‘ì—…ì„ ìœ„í•œ ê³ í’ˆì§ˆ ì‘ë‹µ

## ğŸ“Š Memory Usage Analysis

### Individual Model Testing (gpu-memory-utilization=0.9)
When running each model separately with 90% GPU allocation:

| Model | Parameters | Theoretical Size | Actual Usage | Notes |
|-------|------------|-----------------|--------------|-------|
| TinyLlama 1.1B | 1.1B | 2.2GB | **29.5GB** | Uses maximum available memory |
| Qwen 2.5-3B | 3B | 6GB | **29.6GB** | KV cache expands to fill space |
| Yi-6B | 6B | 12GB | **29.5GB** | Similar behavior across all models |

### Multi-Model Concurrent Execution
When running all three models simultaneously:

| Model | GPU Memory | Percentage | Response Time |
|-------|------------|------------|---------------|
| TinyLlama | 5.3GB | 16% | 33ms |
| Qwen 2.5 | 8.4GB | 26% | 81ms |
| Yi-6B | 17.2GB | 53% | 96ms |
| **Total** | **31.5GB** | **96.6%** | - |

## âš ï¸ Key Findings

### 1. gpu-memory-utilization Behavior
- **Common Misconception**: `gpu-memory-utilization=0.5` means 50% of total GPU memory
- **Reality**: It controls the percentage of *available* memory allocated to KV cache
- **Model weights are always loaded separately** and not included in this calculation

### 2. Memory Allocation Formula
```
Actual Memory = Model Weights + (Available Memory Ã— gpu_utilization) + Overhead
```

### 3. Multi-Model Challenges
- Each container independently checks GPU memory
- Containers are unaware of each other's existence
- Can lead to memory conflicts if not carefully configured

## ğŸ”§ Configuration

### Optimized Settings (docker-compose-balanced.yml)
```yaml
# TinyLlama - Minimal allocation for fast responses
--gpu-memory-utilization 0.15
--max-model-len 1024
--max-num-seqs 32

# Qwen - Balanced allocation for general tasks
--gpu-memory-utilization 0.25
--max-model-len 1536
--max-num-seqs 24

# Yi - Maximum allocation for quality
--gpu-memory-utilization 0.50
--max-model-len 2048
--max-num-seqs 16
```

## ğŸ“ Best Practices

### 1. Start Order Matters
- Start smaller models first, then larger ones
- Use `depends_on` in Docker Compose to ensure proper sequencing

### 2. Parameter Tuning
- **gpu-memory-utilization**: Controls KV cache allocation
- **max-model-len**: Limits context length (reduces KV cache)
- **max-num-seqs**: Limits concurrent requests
- **dtype**: Use `float16` for predictable memory usage

### 3. Monitoring
```bash
# Real-time GPU monitoring
nvidia-smi -l 1

# Check container logs
docker logs vllm-tinyllama --tail 20
```

## ğŸš€ Quick Start

### Prerequisites
- NVIDIA RTX 5090 or similar GPU with 32GB+ VRAM
- Docker with NVIDIA Container Toolkit
- CUDA 12.8+ compatible drivers

### Run All Models
```bash
# Start all three models
docker compose -f docker-compose-balanced.yml up -d

# Check status
docker ps | grep vllm

# Test endpoints
curl -X POST http://localhost:8001/v1/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "tinyllama", "prompt": "Hello", "max_tokens": 10}'
```

### Endpoints
- TinyLlama: `http://localhost:8001`
- Qwen: `http://localhost:8002`
- Yi: `http://localhost:8003`

## ğŸ“ˆ Performance Metrics

| Metric | Value |
|--------|-------|
| Total GPU Memory Used | 31.5GB / 32.6GB |
| GPU Utilization | 96.6% |
| Average Response Time (TinyLlama) | 33ms |
| Average Response Time (Qwen) | 81ms |
| Average Response Time (Yi) | 96ms |
| Concurrent Model Support | 3 models |

## ğŸ“š Comprehensive Reports / ì¢…í•© ë³´ê³ ì„œ

### Available Documentation / ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ì„œ
- ğŸ“Š **[Final Project Report](FINAL_PROJECT_REPORT.md)** - Complete analysis with 4,984+ data points / 4,984ê°œ ì´ìƒì˜ ë°ì´í„° í¬ì¸íŠ¸ê°€ í¬í•¨ëœ ì™„ì „í•œ ë¶„ì„
- ğŸ **[Benchmark Results](FINAL_BENCHMARK_REPORT.md)** - Detailed performance comparisons / ìƒì„¸í•œ ì„±ëŠ¥ ë¹„êµ
- ğŸ” **[Code Analysis](claudedocs/CODE_ANALYSIS_REPORT.md)** - Security and quality assessment / ë³´ì•ˆ ë° í’ˆì§ˆ í‰ê°€
- ğŸ“ˆ **[Multi-model Comparison](MULTIMODEL_SERVING_COMPARISON.md)** - Framework capabilities matrix / í”„ë ˆì„ì›Œí¬ ê¸°ëŠ¥ ë§¤íŠ¸ë¦­ìŠ¤

## ğŸ¯ Use Case Considerations / ì‚¬ìš© ì‚¬ë¡€ ê³ ë ¤ì‚¬í•­

| Scenario / ì‹œë‚˜ë¦¬ì˜¤ | Options / ì˜µì…˜ | Considerations / ê³ ë ¤ì‚¬í•­ |
|--------------------|----------------|--------------------------|
| **Production API** | vLLM, Ollama | vLLM: feature-rich; Ollama: simpler deployment |
| **Development** | Ollama, vLLM | Both offer different advantages |
| **Memory Limited** | Ollama | Dynamic loading reduces memory footprint |
| **Multi-model** | vLLM | Most mature multi-model support |

## ğŸ”¬ Technical Highlights / ê¸°ìˆ ì  í•˜ì´ë¼ì´íŠ¸

### vLLM Characteristics / vLLM íŠ¹ì„±
- Multi-model support with static allocation / ì •ì  í• ë‹¹ìœ¼ë¡œ ë©€í‹°ëª¨ë¸ ì§€ì›
- OpenAI API compatible endpoints / OpenAI API í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸
- Production-ready features / í”„ë¡œë•ì…˜ ì¤€ë¹„ ê¸°ëŠ¥
- Higher memory usage per model / ëª¨ë¸ë‹¹ ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©

### Ollama Characteristics / Ollama íŠ¹ì„±
- Dynamic model loading and unloading / ë™ì  ëª¨ë¸ ë¡œë”© ë° ì–¸ë¡œë”©
- Lower memory footprint / ë‚®ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- Simple deployment process / ê°„ë‹¨í•œ ë°°í¬ í”„ë¡œì„¸ìŠ¤
- CPU fallback capability / CPU í´ë°± ê¸°ëŠ¥

### SGLang Characteristics / SGLang íŠ¹ì„±
- Specialized optimization features / íŠ¹í™”ëœ ìµœì í™” ê¸°ëŠ¥
- Lower baseline memory usage / ë‚®ì€ ê¸°ì¤€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- RTX 5090 compatibility challenges / RTX 5090 í˜¸í™˜ì„± ê³¼ì œ
- Limited multi-model support / ì œí•œì  ë©€í‹°ëª¨ë¸ ì§€ì›

## ğŸ” Security Note / ë³´ì•ˆ ì°¸ê³ ì‚¬í•­

âš ï¸ **Important**: The code analysis found critical security issues that need immediate attention:
- SSH private key exposure - remove immediately
- Hardcoded credentials - use environment variables

âš ï¸ **ì¤‘ìš”**: ì½”ë“œ ë¶„ì„ì—ì„œ ì¦‰ê°ì ì¸ ì£¼ì˜ê°€ í•„ìš”í•œ ì¤‘ìš”í•œ ë³´ì•ˆ ë¬¸ì œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:
- SSH ê°œì¸í‚¤ ë…¸ì¶œ - ì¦‰ì‹œ ì œê±°
- í•˜ë“œì½”ë”©ëœ ìê²©ì¦ëª… - í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©

## ğŸ“ˆ Performance Statistics / ì„±ëŠ¥ í†µê³„

### Test Scale / í…ŒìŠ¤íŠ¸ ê·œëª¨
- **4,984** total data points / ì´ ë°ì´í„° í¬ì¸íŠ¸
- **Multiple benchmark sessions** / ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ ì„¸ì…˜
- **3 frameworks** Ã— **3 models** Ã— **3 languages** / 3ê°œ í”„ë ˆì„ì›Œí¬ Ã— 3ê°œ ëª¨ë¸ Ã— 3ê°œ ì–¸ì–´
- **15 different** configurations tested / 15ê°€ì§€ ë‹¤ë¥¸ êµ¬ì„± í…ŒìŠ¤íŠ¸ë¨

## ğŸ¤ Contributing / ê¸°ì—¬í•˜ê¸°
Feel free to submit issues or PRs with improvements for different GPU configurations.
ë‹¤ë¥¸ GPU êµ¬ì„±ì— ëŒ€í•œ ê°œì„  ì‚¬í•­ê³¼ í•¨ê»˜ ì´ìŠˆë‚˜ PRì„ ììœ ë¡­ê²Œ ì œì¶œí•˜ì„¸ìš”.

## ğŸ“„ License / ë¼ì´ì„¼ìŠ¤
MIT

---
*Tested on / í…ŒìŠ¤íŠ¸ í™˜ê²½: NVIDIA GeForce RTX 5090 (32GB) | CUDA 12.9 | vLLM v0.10.1.1 | SGLang custom | Ollama latest*
*Total benchmark data points / ì´ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° í¬ì¸íŠ¸: 4,984*
*Analysis completed / ë¶„ì„ ì™„ë£Œ: 2025-09-18*