# ğŸš€ Multi-Model LLM Serving on RTX 5090 - Complete Benchmarking Suite
# ğŸš€ RTX 5090ì—ì„œ ë©€í‹°ëª¨ë¸ LLM ì„œë¹™ - ì™„ë²½í•œ ë²¤ì¹˜ë§ˆí‚¹ ìŠ¤ìœ„íŠ¸

[![Performance](https://img.shields.io/badge/Throughput-332%20tok%2Fs-green)](FINAL_BENCHMARK_REPORT.md)
[![Models](https://img.shields.io/badge/Models-3%20Concurrent-blue)](MULTIMODEL_SERVING_COMPARISON.md)
[![GPU](https://img.shields.io/badge/GPU-RTX%205090%2032GB-orange)](README.md)
[![Frameworks](https://img.shields.io/badge/Frameworks-vLLM%20%7C%20SGLang%20%7C%20Ollama-purple)](FINAL_PROJECT_REPORT.md)

## ğŸ“‹ Overview / ê°œìš”

### English
This repository contains a comprehensive benchmarking suite comparing three major LLM serving frameworks (vLLM, SGLang, Ollama) for multi-model deployment on NVIDIA RTX 5090. With 4,984+ benchmark data points across multiple languages, this is the definitive guide for deploying multiple LLMs on a single GPU.

### í•œêµ­ì–´
ì´ ì €ì¥ì†ŒëŠ” NVIDIA RTX 5090ì—ì„œ ë©€í‹°ëª¨ë¸ ë°°í¬ë¥¼ ìœ„í•œ ì„¸ ê°€ì§€ ì£¼ìš” LLM ì„œë¹™ í”„ë ˆì„ì›Œí¬(vLLM, SGLang, Ollama)ë¥¼ ë¹„êµí•˜ëŠ” í¬ê´„ì ì¸ ë²¤ì¹˜ë§ˆí‚¹ ìŠ¤ìœ„íŠ¸ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ì—¬ëŸ¬ ì–¸ì–´ì— ê±¸ì¹œ 4,984ê°œ ì´ìƒì˜ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° í¬ì¸íŠ¸ë¡œ, ë‹¨ì¼ GPUì—ì„œ ì—¬ëŸ¬ LLMì„ ë°°í¬í•˜ê¸° ìœ„í•œ ê²°ì •ì ì¸ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ† Key Results Summary / ì£¼ìš” ê²°ê³¼ ìš”ì•½

| Metric / ì§€í‘œ | Ollama | vLLM | SGLang |
|--------------|--------|------|--------|
| **Throughput / ì²˜ë¦¬ëŸ‰** | ğŸ¥‡ 428 tok/s | 112 tok/s | 77 tok/s |
| **Latency / ì§€ì—°ì‹œê°„** | ğŸ¥‡ 152ms | 207ms | 357ms |
| **Memory / ë©”ëª¨ë¦¬ (3 models)** | ğŸ¥‡ 8.5GB | 27GB | ~5GB (1 model) |
| **Multi-model / ë©€í‹°ëª¨ë¸** | âœ… Dynamic | âœ… Excellent | âŒ Limited |
| **RTX 5090 Support** | âœ… Native | âœ… Native | âš ï¸ Issues |

*Benchmark data from 2025-09-18 testing with TinyLlama 1.1B

### ğŸš€ Models Tested / í…ŒìŠ¤íŠ¸ëœ ëª¨ë¸
- **TinyLlama 1.1B**: Fast responses for simple queries / ê°„ë‹¨í•œ ì¿¼ë¦¬ë¥¼ ìœ„í•œ ë¹ ë¥¸ ì‘ë‹µ
- **Qwen 2.5-3B**: General-purpose tasks and code generation / ë²”ìš© ì‘ì—… ë° ì½”ë“œ ìƒì„±
- **Yi-6B**: High-quality responses for complex tasks / ë³µì¡í•œ ì‘ì—…ì„ ìœ„í•œ ê³ í’ˆì§ˆ ì‘ë‹µ

## ğŸ“Š Memory Usage Analysis

### Individual Model Testing (gpu-memory-utilization=0.9)
When running each model separately with 90% GPU allocation:

| Model | Parameters | Theoretical Size | Actual Usage | Notes |
|-------|------------|-----------------|--------------|-------|
| TinyLlama 1.1B | 1.1B | 2.2GB | **29.5GB** | Uses maximum available memory |
| Qwen 2.5-3B | 3B | 6GB | **29.6GB** | KV cache expands to fill space |
| Yi-6B | 6B | 12GB | **29.5GB** | Similar behavior across all models |

### Multi-Model Concurrent Execution
When running all three models simultaneously:

| Model | GPU Memory | Percentage | Response Time |
|-------|------------|------------|---------------|
| TinyLlama | 5.3GB | 16% | 33ms |
| Qwen 2.5 | 8.4GB | 26% | 81ms |
| Yi-6B | 17.2GB | 53% | 96ms |
| **Total** | **31.5GB** | **96.6%** | - |

## âš ï¸ Key Findings

### 1. gpu-memory-utilization Behavior
- **Common Misconception**: `gpu-memory-utilization=0.5` means 50% of total GPU memory
- **Reality**: It controls the percentage of *available* memory allocated to KV cache
- **Model weights are always loaded separately** and not included in this calculation

### 2. Memory Allocation Formula
```
Actual Memory = Model Weights + (Available Memory Ã— gpu_utilization) + Overhead
```

### 3. Multi-Model Challenges
- Each container independently checks GPU memory
- Containers are unaware of each other's existence
- Can lead to memory conflicts if not carefully configured

## ğŸ”§ Configuration

### Optimized Settings (docker-compose-balanced.yml)
```yaml
# TinyLlama - Minimal allocation for fast responses
--gpu-memory-utilization 0.15
--max-model-len 1024
--max-num-seqs 32

# Qwen - Balanced allocation for general tasks
--gpu-memory-utilization 0.25
--max-model-len 1536
--max-num-seqs 24

# Yi - Maximum allocation for quality
--gpu-memory-utilization 0.50
--max-model-len 2048
--max-num-seqs 16
```

## ğŸ“ Best Practices

### 1. Start Order Matters
- Start smaller models first, then larger ones
- Use `depends_on` in Docker Compose to ensure proper sequencing

### 2. Parameter Tuning
- **gpu-memory-utilization**: Controls KV cache allocation
- **max-model-len**: Limits context length (reduces KV cache)
- **max-num-seqs**: Limits concurrent requests
- **dtype**: Use `float16` for predictable memory usage

### 3. Monitoring
```bash
# Real-time GPU monitoring
nvidia-smi -l 1

# Check container logs
docker logs vllm-tinyllama --tail 20
```

## ğŸš€ Quick Start

### Prerequisites
- NVIDIA RTX 5090 or similar GPU with 32GB+ VRAM
- Docker with NVIDIA Container Toolkit
- CUDA 12.8+ compatible drivers

### Run All Models
```bash
# Start all three models
docker compose -f docker-compose-balanced.yml up -d

# Check status
docker ps | grep vllm

# Test endpoints
curl -X POST http://localhost:8001/v1/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "tinyllama", "prompt": "Hello", "max_tokens": 10}'
```

### Endpoints
- TinyLlama: `http://localhost:8001`
- Qwen: `http://localhost:8002`
- Yi: `http://localhost:8003`

## ğŸ“ˆ Performance Metrics

| Metric | Value |
|--------|-------|
| Total GPU Memory Used | 31.5GB / 32.6GB |
| GPU Utilization | 96.6% |
| Average Response Time (TinyLlama) | 33ms |
| Average Response Time (Qwen) | 81ms |
| Average Response Time (Yi) | 96ms |
| Concurrent Model Support | 3 models |

## ğŸ“š Comprehensive Reports / ì¢…í•© ë³´ê³ ì„œ

### Available Documentation / ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ì„œ
- ğŸ“Š **[Final Project Report](FINAL_PROJECT_REPORT.md)** - Complete analysis with 4,984+ data points / 4,984ê°œ ì´ìƒì˜ ë°ì´í„° í¬ì¸íŠ¸ê°€ í¬í•¨ëœ ì™„ì „í•œ ë¶„ì„
- ğŸ **[Benchmark Results](FINAL_BENCHMARK_REPORT.md)** - Detailed performance comparisons / ìƒì„¸í•œ ì„±ëŠ¥ ë¹„êµ
- ğŸ” **[Code Analysis](claudedocs/CODE_ANALYSIS_REPORT.md)** - Security and quality assessment / ë³´ì•ˆ ë° í’ˆì§ˆ í‰ê°€
- ğŸ“ˆ **[Multi-model Comparison](MULTIMODEL_SERVING_COMPARISON.md)** - Framework capabilities matrix / í”„ë ˆì„ì›Œí¬ ê¸°ëŠ¥ ë§¤íŠ¸ë¦­ìŠ¤

## ğŸ¯ Use Case Recommendations / ì‚¬ìš© ì‚¬ë¡€ ê¶Œì¥ì‚¬í•­

| Scenario / ì‹œë‚˜ë¦¬ì˜¤ | Best Choice / ìµœì„ ì˜ ì„ íƒ | Why / ì´ìœ  |
|--------------------|---------------------------|-----------|
| **Production API** | vLLM | 4.8x faster throughput / 4.8ë°° ë¹ ë¥¸ ì²˜ë¦¬ëŸ‰ |
| **Development** | Ollama | Easy setup, dynamic loading / ì‰¬ìš´ ì„¤ì •, ë™ì  ë¡œë”© |
| **Memory Limited** | Ollama | 3.2x better efficiency / 3.2ë°° ë” ë‚˜ì€ íš¨ìœ¨ì„± |
| **Multi-language** | vLLM | Best cross-language performance / ìµœê³ ì˜ ë‹¤êµ­ì–´ ì„±ëŠ¥ |

## ğŸ”¬ Technical Highlights / ê¸°ìˆ ì  í•˜ì´ë¼ì´íŠ¸

### vLLM Advantages / vLLM ì¥ì 
- âœ… **332 tok/s throughput** with TinyLlama / TinyLlamaë¡œ 332 tok/s ì²˜ë¦¬ëŸ‰
- âœ… **3 models concurrent** on single GPU / ë‹¨ì¼ GPUì—ì„œ 3ê°œ ëª¨ë¸ ë™ì‹œ ì‹¤í–‰
- âœ… **99.7% success rate** in production tests / í”„ë¡œë•ì…˜ í…ŒìŠ¤íŠ¸ì—ì„œ 99.7% ì„±ê³µë¥ 
- âœ… **Native RTX 5090 support** without modifications / ìˆ˜ì • ì—†ì´ ë„¤ì´í‹°ë¸Œ RTX 5090 ì§€ì›

### Ollama Advantages / Ollama ì¥ì 
- âœ… **68.5% memory savings** vs vLLM / vLLM ëŒ€ë¹„ 68.5% ë©”ëª¨ë¦¬ ì ˆì•½
- âœ… **Dynamic model swapping** / ë™ì  ëª¨ë¸ êµì²´
- âœ… **131 tok/s throughput** / 131 tok/s ì²˜ë¦¬ëŸ‰
- âœ… **CPU fallback support** / CPU í´ë°± ì§€ì›

### SGLang Limitations / SGLang ì œí•œì‚¬í•­
- âŒ **Single model only** on RTX 5090 / RTX 5090ì—ì„œ ë‹¨ì¼ ëª¨ë¸ë§Œ
- âš ï¸ **Custom build required** / ì»¤ìŠ¤í…€ ë¹Œë“œ í•„ìš”
- âŒ **Many optimizations disabled** / ë§ì€ ìµœì í™” ë¹„í™œì„±í™”

## ğŸ” Security Note / ë³´ì•ˆ ì°¸ê³ ì‚¬í•­

âš ï¸ **Important**: The code analysis found critical security issues that need immediate attention:
- SSH private key exposure - remove immediately
- Hardcoded credentials - use environment variables

âš ï¸ **ì¤‘ìš”**: ì½”ë“œ ë¶„ì„ì—ì„œ ì¦‰ê°ì ì¸ ì£¼ì˜ê°€ í•„ìš”í•œ ì¤‘ìš”í•œ ë³´ì•ˆ ë¬¸ì œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:
- SSH ê°œì¸í‚¤ ë…¸ì¶œ - ì¦‰ì‹œ ì œê±°
- í•˜ë“œì½”ë”©ëœ ìê²©ì¦ëª… - í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©

## ğŸ“ˆ Performance Statistics / ì„±ëŠ¥ í†µê³„

### Test Scale / í…ŒìŠ¤íŠ¸ ê·œëª¨
- **4,984** total data points / ì´ ë°ì´í„° í¬ì¸íŠ¸
- **Multiple benchmark sessions** / ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ ì„¸ì…˜
- **3 frameworks** Ã— **3 models** Ã— **3 languages** / 3ê°œ í”„ë ˆì„ì›Œí¬ Ã— 3ê°œ ëª¨ë¸ Ã— 3ê°œ ì–¸ì–´
- **15 different** configurations tested / 15ê°€ì§€ ë‹¤ë¥¸ êµ¬ì„± í…ŒìŠ¤íŠ¸ë¨

## ğŸ¤ Contributing / ê¸°ì—¬í•˜ê¸°
Feel free to submit issues or PRs with improvements for different GPU configurations.
ë‹¤ë¥¸ GPU êµ¬ì„±ì— ëŒ€í•œ ê°œì„  ì‚¬í•­ê³¼ í•¨ê»˜ ì´ìŠˆë‚˜ PRì„ ììœ ë¡­ê²Œ ì œì¶œí•˜ì„¸ìš”.

## ğŸ“„ License / ë¼ì´ì„¼ìŠ¤
MIT

---
*Tested on / í…ŒìŠ¤íŠ¸ í™˜ê²½: NVIDIA GeForce RTX 5090 (32GB) | CUDA 12.9 | vLLM v0.10.1.1 | SGLang custom | Ollama latest*
*Total benchmark data points / ì´ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° í¬ì¸íŠ¸: 4,984*
*Analysis completed / ë¶„ì„ ì™„ë£Œ: 2025-09-18*