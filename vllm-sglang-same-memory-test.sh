#!/bin/bash

echo "ðŸ”¬ vLLM vs SGLang ë™ì¼ ë©”ëª¨ë¦¬ ì œí•œ í…ŒìŠ¤íŠ¸"
echo "ëª©í‘œ: ê° í”„ë ˆìž„ì›Œí¬ê°€ ë™ì¼í•œ GPU ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •"
echo ""

# ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬
echo "ðŸ§¹ ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬..."
docker stop $(docker ps -q --filter "name=vllm") 2>/dev/null
docker stop $(docker ps -q --filter "name=sglang") 2>/dev/null
docker rm $(docker ps -aq --filter "name=vllm") 2>/dev/null
docker rm $(docker ps -aq --filter "name=sglang") 2>/dev/null
sleep 5

echo "ðŸ“Š ì´ˆê¸° GPU ìƒíƒœ:"
nvidia-smi --query-gpu=memory.used,memory.free,memory.total --format=csv

# SGLang í…ŒìŠ¤íŠ¸ - ì „ì²´ GPUì˜ 15% ì‚¬ìš© (ì•½ 4.9GB)
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "1ï¸âƒ£ SGLang í…ŒìŠ¤íŠ¸ (mem-fraction-static: 0.15 = 4.9GB)"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

docker run -d \
  --name sglang-15pct \
  --runtime nvidia \
  --gpus all \
  -p 30001:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  -e PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True" \
  --shm-size 8g \
  sglang:blackwell-final-v2 \
  --model-path TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --host 0.0.0.0 \
  --port 8000 \
  --mem-fraction-static 0.15 \
  --max-total-tokens 2048 \
  --dtype float16 \
  --trust-remote-code \
  --disable-cuda-graph \
  --disable-custom-all-reduce \
  --disable-flashinfer \
  --disable-radix-cache \
  --attention-backend torch_native

echo "â³ SGLang ì´ˆê¸°í™” ì¤‘ (40ì´ˆ)..."
sleep 40

echo "SGLang ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:"
nvidia-smi --query-gpu=memory.used --format=csv
SGLANG_MEM=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits)
echo "SGLangì´ ì‚¬ìš© ì¤‘ì¸ ë©”ëª¨ë¦¬: ${SGLANG_MEM}MB"

# SGLang ì¢…ë£Œ
docker stop sglang-15pct 2>/dev/null
docker rm sglang-15pct 2>/dev/null
sleep 5

# vLLM í…ŒìŠ¤íŠ¸ - ë™ì¼í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©í•˜ë„ë¡ ì¡°ì •
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "2ï¸âƒ£ vLLM í…ŒìŠ¤íŠ¸ (ë™ì¼ ë©”ëª¨ë¦¬ ëª©í‘œ: ~${SGLANG_MEM}MB)"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

# vLLMì€ ê°€ìš© ë©”ëª¨ë¦¬ ê¸°ì¤€ì´ë¯€ë¡œ ì „ì²´ 32GB ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
# SGLang 15% â‰ˆ 4.9GB ì‚¬ìš©
# vLLMì—ì„œ ë¹„ìŠ·í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ìœ„í•´ gpu-memory-utilization ì¡°ì •

echo "í…ŒìŠ¤íŠ¸ A: vLLM gpu-memory-utilization=0.10"
docker run -d \
  --name vllm-test-a \
  --runtime nvidia \
  --gpus all \
  -p 40001:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  --shm-size 8g \
  vllm/vllm-openai:latest \
  --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --host 0.0.0.0 \
  --port 8000 \
  --gpu-memory-utilization 0.10 \
  --max-model-len 2048 \
  --enforce-eager

sleep 40
echo "vLLM (0.10) ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:"
nvidia-smi --query-gpu=memory.used --format=csv
VLLM_MEM_A=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits)

docker stop vllm-test-a 2>/dev/null
docker rm vllm-test-a 2>/dev/null
sleep 5

echo ""
echo "í…ŒìŠ¤íŠ¸ B: vLLM gpu-memory-utilization=0.15"
docker run -d \
  --name vllm-test-b \
  --runtime nvidia \
  --gpus all \
  -p 40001:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  --shm-size 8g \
  vllm/vllm-openai:latest \
  --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --host 0.0.0.0 \
  --port 8000 \
  --gpu-memory-utilization 0.15 \
  --max-model-len 2048 \
  --enforce-eager

sleep 40
echo "vLLM (0.15) ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:"
nvidia-smi --query-gpu=memory.used --format=csv
VLLM_MEM_B=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits)

echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ðŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "SGLang (mem-fraction-static=0.15): ${SGLANG_MEM}MB"
echo "vLLM (gpu-memory-utilization=0.10): ${VLLM_MEM_A}MB"
echo "vLLM (gpu-memory-utilization=0.15): ${VLLM_MEM_B}MB"
echo ""
echo "ðŸ’¡ ë¶„ì„:"
echo "- SGLangì˜ mem-fraction-staticì€ ì „ì²´ GPU ë©”ëª¨ë¦¬(32GB)ì˜ ë¹„ìœ¨"
echo "- vLLMì˜ gpu-memory-utilizationì€ ê°€ìš© ë©”ëª¨ë¦¬ì˜ ë¹„ìœ¨"
echo "- ë™ì¼í•œ 0.15 ê°’ì´ì–´ë„ ì‹¤ì œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì€ ë‹¤ë¦„"

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "âš¡ ì„±ëŠ¥ ë¹„êµ (ë™ì¼ ë©”ëª¨ë¦¬ ì‚¬ìš© ì‹œ)"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

# ê°„ë‹¨í•œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
echo "vLLM ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸..."
START=$(date +%s%N)
curl -s -X POST http://localhost:40001/v1/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "prompt": "Hello, how are you?", "max_tokens": 50}' > /dev/null
END=$(date +%s%N)
VLLM_TIME=$((($END - $START) / 1000000))
echo "vLLM ì‘ë‹µ ì‹œê°„: ${VLLM_TIME}ms"

docker stop vllm-test-b 2>/dev/null
docker rm vllm-test-b 2>/dev/null

# ì •ë¦¬
docker stop $(docker ps -q --filter "name=vllm") 2>/dev/null
docker stop $(docker ps -q --filter "name=sglang") 2>/dev/null