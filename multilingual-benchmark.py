#!/usr/bin/env python3
"""
Multilingual Performance Benchmark for SGLang vs vLLM
Tests with Chinese, English, and Korean prompts about seasons
"""

import time
import json
import requests
import csv
import statistics
import numpy as np
from datetime import datetime
import subprocess
import sys
from typing import List, Dict, Any
import pandas as pd

# ë‹¤êµ­ì–´ í”„ë¡¬í”„íŠ¸ - ê³„ì ˆì„ ë…¸ë˜í•˜ë¼
PROMPTS = {
    "english": [
        "Sing a song about the beauty of spring with blooming flowers.",
        "Create a melody about the warm summer days at the beach.",
        "Compose lyrics about autumn leaves falling gently.",
        "Write a song about winter snowflakes dancing in the air.",
        "Sing about the four seasons changing throughout the year."
    ],
    "chinese": [
        "å”±ä¸€é¦–å…³äºæ˜¥å¤©ç™¾èŠ±ç››å¼€ä¹‹ç¾çš„æ­Œæ›²ã€‚",
        "åˆ›ä½œä¸€é¦–å…³äºå¤æ—¥æµ·æ»©æ¸©æš–æ—¶å…‰çš„æ—‹å¾‹ã€‚",
        "è°±å†™å…³äºç§‹å¶è½»è½»é£˜è½çš„æ­Œè¯ã€‚",
        "å†™ä¸€é¦–å…³äºå†¬å¤©é›ªèŠ±åœ¨ç©ºä¸­é£èˆçš„æ­Œã€‚",
        "æ­Œå”±å››å­£æ›´æ›¿çš„ç¾å¦™å˜åŒ–ã€‚"
    ],
    "korean": [
        "ë´„ì— í”¼ì–´ë‚˜ëŠ” ê½ƒë“¤ì˜ ì•„ë¦„ë‹¤ì›€ì„ ë…¸ë˜í•´ì£¼ì„¸ìš”.",
        "í•´ë³€ì—ì„œ ë³´ë‚´ëŠ” ë”°ëœ»í•œ ì—¬ë¦„ë‚ ì˜ ë©œë¡œë””ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.",
        "ê°€ì„ ë‚™ì—½ì´ ë¶€ë“œëŸ½ê²Œ ë–¨ì–´ì§€ëŠ” ê²ƒì— ëŒ€í•œ ê°€ì‚¬ë¥¼ ì¨ì£¼ì„¸ìš”.",
        "ê²¨ìš¸ ëˆˆì†¡ì´ê°€ ê³µì¤‘ì—ì„œ ì¶¤ì¶”ëŠ” ê²ƒì— ëŒ€í•œ ë…¸ë˜ë¥¼ ì¨ì£¼ì„¸ìš”.",
        "ì¼ë…„ ë‚´ë‚´ ë³€í™”í•˜ëŠ” ì‚¬ê³„ì ˆì„ ë…¸ë˜í•´ì£¼ì„¸ìš”."
    ]
}

# í…ŒìŠ¤íŠ¸ ì„¤ì •
NUM_ITERATIONS = 10  # ê° í”„ë¡¬í”„íŠ¸ë‹¹ í…ŒìŠ¤íŠ¸ íšŸìˆ˜ (ì´ 10 * 15 = 150 tests per framework) - ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©
OUTPUT_TOKENS = 100   # ìƒì„±í•  í† í° ìˆ˜

class BenchmarkRunner:
    def __init__(self):
        self.results = []

    def test_sglang(self, prompt: str, language: str, prompt_idx: int, iteration: int) -> Dict[str, Any]:
        """SGLang API í…ŒìŠ¤íŠ¸"""
        url = "http://localhost:30001/generate"
        payload = {
            "text": prompt,
            "max_new_tokens": OUTPUT_TOKENS,
            "temperature": 0.7,
            "top_p": 0.9
        }

        # Time to First Token (TTFT) ì¸¡ì •
        start_time = time.perf_counter()
        try:
            response = requests.post(url, json=payload, timeout=60)
            response.raise_for_status()
            first_byte_time = time.perf_counter()

            result = response.json()
            end_time = time.perf_counter()

            generated_text = result.get("text", "")
            tokens = len(generated_text.split())

            ttft = (first_byte_time - start_time) * 1000  # ms
            total_latency = (end_time - start_time) * 1000  # ms
            generation_latency = total_latency - ttft

            return {
                "framework": "SGLang",
                "language": language,
                "prompt_idx": prompt_idx,
                "iteration": iteration,
                "success": True,
                "ttft_ms": ttft,
                "generation_latency_ms": generation_latency,
                "total_latency_ms": total_latency,
                "output_tokens": tokens,
                "tokens_per_second": tokens / (total_latency / 1000) if total_latency > 0 else 0,
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            return {
                "framework": "SGLang",
                "language": language,
                "prompt_idx": prompt_idx,
                "iteration": iteration,
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    def test_vllm(self, prompt: str, language: str, prompt_idx: int, iteration: int) -> Dict[str, Any]:
        """vLLM API í…ŒìŠ¤íŠ¸"""
        url = "http://localhost:40001/v1/completions"
        payload = {
            "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "prompt": prompt,
            "max_tokens": OUTPUT_TOKENS,
            "temperature": 0.7,
            "top_p": 0.9,
            "stream": False
        }

        start_time = time.perf_counter()
        try:
            response = requests.post(url, json=payload, timeout=60)
            response.raise_for_status()
            first_byte_time = time.perf_counter()

            result = response.json()
            end_time = time.perf_counter()

            generated_text = result["choices"][0]["text"]
            tokens = result["usage"]["completion_tokens"]

            ttft = (first_byte_time - start_time) * 1000  # ms
            total_latency = (end_time - start_time) * 1000  # ms
            generation_latency = total_latency - ttft

            return {
                "framework": "vLLM",
                "language": language,
                "prompt_idx": prompt_idx,
                "iteration": iteration,
                "success": True,
                "ttft_ms": ttft,
                "generation_latency_ms": generation_latency,
                "total_latency_ms": total_latency,
                "output_tokens": tokens,
                "tokens_per_second": tokens / (total_latency / 1000) if total_latency > 0 else 0,
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            return {
                "framework": "vLLM",
                "language": language,
                "prompt_idx": prompt_idx,
                "iteration": iteration,
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    def run_benchmark(self, framework: str) -> List[Dict]:
        """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print(f"\n{'='*80}")
        print(f"ğŸš€ {framework} Multilingual Benchmark Starting")
        print(f"Languages: Chinese, English, Korean")
        print(f"Iterations per prompt: {NUM_ITERATIONS}")
        print(f"{'='*80}\n")

        results = []
        total_tests = len(PROMPTS) * 5 * NUM_ITERATIONS  # 3 languages * 5 prompts * iterations
        current_test = 0

        for language, prompt_list in PROMPTS.items():
            print(f"\nğŸ“ Testing {language.upper()} prompts...")

            for prompt_idx, prompt in enumerate(prompt_list):
                print(f"  Prompt {prompt_idx + 1}: ", end="")
                successful_tests = 0

                for iteration in range(NUM_ITERATIONS):
                    current_test += 1

                    if framework == "SGLang":
                        result = self.test_sglang(prompt, language, prompt_idx, iteration)
                    else:
                        result = self.test_vllm(prompt, language, prompt_idx, iteration)

                    results.append(result)

                    if result["success"]:
                        successful_tests += 1

                    # Progress indicator
                    if (iteration + 1) % 20 == 0:
                        print(f".", end="", flush=True)

                    # Small delay between requests
                    time.sleep(0.05)

                print(f" [{successful_tests}/{NUM_ITERATIONS} success]")

        print(f"\nâœ… {framework} benchmark completed: {len(results)} tests")
        return results

    def save_to_csv(self, results: List[Dict], filename: str):
        """ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        df = pd.DataFrame(results)
        df.to_csv(filename, index=False)
        print(f"ğŸ’¾ Results saved to {filename}")
        return df

    def analyze_results(self, sglang_df: pd.DataFrame, vllm_df: pd.DataFrame) -> Dict:
        """ê²°ê³¼ ë¶„ì„"""
        analysis = {}

        for framework, df in [("SGLang", sglang_df), ("vLLM", vllm_df)]:
            # ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ë§Œ ë¶„ì„
            success_df = df[df['success'] == True]

            analysis[framework] = {
                "total_tests": len(df),
                "successful_tests": len(success_df),
                "success_rate": len(success_df) / len(df) * 100,

                "overall": {
                    "avg_ttft_ms": success_df['ttft_ms'].mean(),
                    "p50_ttft_ms": success_df['ttft_ms'].median(),
                    "p95_ttft_ms": success_df['ttft_ms'].quantile(0.95),
                    "p99_ttft_ms": success_df['ttft_ms'].quantile(0.99),

                    "avg_total_latency_ms": success_df['total_latency_ms'].mean(),
                    "p50_total_latency_ms": success_df['total_latency_ms'].median(),
                    "p95_total_latency_ms": success_df['total_latency_ms'].quantile(0.95),
                    "p99_total_latency_ms": success_df['total_latency_ms'].quantile(0.99),

                    "avg_tokens_per_second": success_df['tokens_per_second'].mean(),
                    "max_tokens_per_second": success_df['tokens_per_second'].max(),
                    "min_tokens_per_second": success_df['tokens_per_second'].min(),
                },

                "by_language": {}
            }

            # ì–¸ì–´ë³„ ë¶„ì„
            for language in ['english', 'chinese', 'korean']:
                lang_df = success_df[success_df['language'] == language]
                if len(lang_df) > 0:
                    analysis[framework]["by_language"][language] = {
                        "count": len(lang_df),
                        "avg_ttft_ms": lang_df['ttft_ms'].mean(),
                        "avg_latency_ms": lang_df['total_latency_ms'].mean(),
                        "avg_tokens_per_second": lang_df['tokens_per_second'].mean(),
                    }

        return analysis

def main():
    print("\n" + "="*80)
    print("ğŸ”¬ Multilingual Performance Benchmark: SGLang vs vLLM")
    print(f"ğŸ“… Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("ğŸ–¥ï¸  GPU: NVIDIA RTX 5090 (32GB)")
    print("ğŸ“ Languages: Chinese, English, Korean")
    print(f"ğŸ”¢ Tests per prompt: {NUM_ITERATIONS}")
    print(f"ğŸ“Š Total tests per framework: {NUM_ITERATIONS * 15}")
    print("="*80)

    runner = BenchmarkRunner()

    # ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬í•˜ê³  ìƒˆë¡œ ì‹œì‘
    print("\nğŸ”„ Setting up test environment...")
    subprocess.run(["docker", "stop", "$(docker ps -q)"], shell=True, capture_output=True)
    time.sleep(5)

    # SGLang ì‹œì‘
    print("Starting SGLang container...")
    subprocess.run(["/home/multi-model-server/deploy-sglang-single.sh"], capture_output=True)
    print("â³ Waiting for SGLang initialization (45s)...")
    time.sleep(45)

    # SGLang ë²¤ì¹˜ë§ˆí¬
    print("\n" + "="*80)
    print("PHASE 1: SGLang Benchmark")
    print("="*80)
    sglang_results = runner.run_benchmark("SGLang")

    # SGLang ì¢…ë£Œ, vLLM ì‹œì‘
    print("\nğŸ”„ Switching to vLLM...")
    subprocess.run(["docker", "stop", "sglang-tinyllama"], capture_output=True)
    time.sleep(5)

    subprocess.run(["bash", "-c", """
        docker run -d \
          --name vllm-benchmark \
          --runtime nvidia \
          --gpus all \
          -p 40001:8000 \
          -v /root/.cache/huggingface:/root/.cache/huggingface \
          --shm-size 8g \
          vllm/vllm-openai:latest \
          --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
          --host 0.0.0.0 \
          --port 8000 \
          --gpu-memory-utilization 0.05 \
          --max-model-len 2048 \
          --enforce-eager
    """], capture_output=True)

    print("â³ Waiting for vLLM initialization (45s)...")
    time.sleep(45)

    # vLLM ë²¤ì¹˜ë§ˆí¬
    print("\n" + "="*80)
    print("PHASE 2: vLLM Benchmark")
    print("="*80)
    vllm_results = runner.run_benchmark("vLLM")

    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    sglang_csv = f"/home/multi-model-server/benchmark_sglang_{timestamp}.csv"
    vllm_csv = f"/home/multi-model-server/benchmark_vllm_{timestamp}.csv"

    print("\n" + "="*80)
    print("ğŸ“Š Saving Results")
    print("="*80)

    sglang_df = runner.save_to_csv(sglang_results, sglang_csv)
    vllm_df = runner.save_to_csv(vllm_results, vllm_csv)

    # ë¶„ì„
    print("\n" + "="*80)
    print("ğŸ“ˆ Analyzing Results")
    print("="*80)

    analysis = runner.analyze_results(sglang_df, vllm_df)

    # ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    for framework in ["SGLang", "vLLM"]:
        print(f"\nğŸ·ï¸  {framework} Results:")
        print(f"  âœ… Success Rate: {analysis[framework]['success_rate']:.1f}%")
        print(f"  âš¡ Avg TTFT: {analysis[framework]['overall']['avg_ttft_ms']:.2f}ms")
        print(f"  âš¡ P50 TTFT: {analysis[framework]['overall']['p50_ttft_ms']:.2f}ms")
        print(f"  âš¡ P95 TTFT: {analysis[framework]['overall']['p95_ttft_ms']:.2f}ms")
        print(f"  ğŸ“Š Avg Throughput: {analysis[framework]['overall']['avg_tokens_per_second']:.1f} tokens/s")

        print(f"\n  By Language:")
        for lang in ['english', 'chinese', 'korean']:
            if lang in analysis[framework]['by_language']:
                stats = analysis[framework]['by_language'][lang]
                print(f"    {lang.capitalize()}:")
                print(f"      - Avg TTFT: {stats['avg_ttft_ms']:.2f}ms")
                print(f"      - Avg Latency: {stats['avg_latency_ms']:.2f}ms")
                print(f"      - Throughput: {stats['avg_tokens_per_second']:.1f} tok/s")

    # ë¶„ì„ ê²°ê³¼ JSON ì €ì¥
    analysis_file = f"/home/multi-model-server/benchmark_analysis_{timestamp}.json"
    with open(analysis_file, "w") as f:
        json.dump(analysis, f, indent=2, default=str)
    print(f"\nğŸ’¾ Analysis saved to {analysis_file}")

    # ì •ë¦¬
    subprocess.run(["docker", "stop", "vllm-benchmark"], capture_output=True)

    print("\nâœ… Benchmark completed successfully!")

    return analysis

if __name__ == "__main__":
    main()