#!/bin/bash

echo "ğŸ”¬ Ollama ë©€í‹°ëª¨ë¸ ì„œë¹™ í…ŒìŠ¤íŠ¸"
echo "================================"
echo ""

# GPU ìƒíƒœ í™•ì¸
echo "ğŸ“Š ì´ˆê¸° GPU ë©”ëª¨ë¦¬ ìƒíƒœ:"
nvidia-smi --query-gpu=memory.used,memory.free --format=csv
echo ""

# TinyLlama í…ŒìŠ¤íŠ¸
echo "1ï¸âƒ£ TinyLlama ëª¨ë¸ í˜¸ì¶œ:"
START=$(date +%s%N)
curl -s -X POST http://localhost:11434/api/generate \
  -d '{
    "model": "tinyllama:1.1b",
    "prompt": "Sing a beautiful song about spring with blooming flowers.",
    "stream": false,
    "options": {"num_predict": 100, "temperature": 0.7}
  }' | jq -r '.response' | head -5
END=$(date +%s%N)
ELAPSED=$((($END - $START) / 1000000))
echo "â±ï¸ TinyLlama ì‘ë‹µ ì‹œê°„: ${ELAPSED}ms"
echo ""

# GPU ìƒíƒœ í™•ì¸
echo "ğŸ“Š TinyLlama ì‹¤í–‰ í›„ GPU ë©”ëª¨ë¦¬:"
nvidia-smi --query-gpu=memory.used,memory.free --format=csv
echo ""

# Qwen í…ŒìŠ¤íŠ¸
echo "2ï¸âƒ£ Qwen 2.5-3B ëª¨ë¸ í˜¸ì¶œ:"
START=$(date +%s%N)
curl -s -X POST http://localhost:11434/api/generate \
  -d '{
    "model": "qwen2.5:3b",
    "prompt": "å”±ä¸€é¦–å…³äºæ˜¥å¤©ç™¾èŠ±ç››å¼€ä¹‹ç¾çš„æ­Œæ›²ã€‚",
    "stream": false,
    "options": {"num_predict": 100, "temperature": 0.7}
  }' | jq -r '.response' | head -5
END=$(date +%s%N)
ELAPSED=$((($END - $START) / 1000000))
echo "â±ï¸ Qwen ì‘ë‹µ ì‹œê°„: ${ELAPSED}ms"
echo ""

# GPU ìƒíƒœ í™•ì¸
echo "ğŸ“Š Qwen ì‹¤í–‰ í›„ GPU ë©”ëª¨ë¦¬:"
nvidia-smi --query-gpu=memory.used,memory.free --format=csv
echo ""

# Yi í…ŒìŠ¤íŠ¸
echo "3ï¸âƒ£ Yi-6B ëª¨ë¸ í˜¸ì¶œ:"
START=$(date +%s%N)
curl -s -X POST http://localhost:11434/api/generate \
  -d '{
    "model": "yi:6b",
    "prompt": "ë´„ì— í”¼ì–´ë‚˜ëŠ” ê½ƒë“¤ì˜ ì•„ë¦„ë‹¤ì›€ì„ ë…¸ë˜í•´ì£¼ì„¸ìš”.",
    "stream": false,
    "options": {"num_predict": 100, "temperature": 0.7}
  }' | jq -r '.response' | head -5
END=$(date +%s%N)
ELAPSED=$((($END - $START) / 1000000))
echo "â±ï¸ Yi ì‘ë‹µ ì‹œê°„: ${ELAPSED}ms"
echo ""

# GPU ìƒíƒœ í™•ì¸
echo "ğŸ“Š Yi ì‹¤í–‰ í›„ GPU ë©”ëª¨ë¦¬:"
nvidia-smi --query-gpu=memory.used,memory.free --format=csv
echo ""

# ë‹¤ì‹œ TinyLlama í˜¸ì¶œ (ëª¨ë¸ ì „í™˜ í…ŒìŠ¤íŠ¸)
echo "4ï¸âƒ£ TinyLlama ì¬í˜¸ì¶œ (ëª¨ë¸ ì „í™˜ í…ŒìŠ¤íŠ¸):"
START=$(date +%s%N)
curl -s -X POST http://localhost:11434/api/generate \
  -d '{
    "model": "tinyllama:1.1b",
    "prompt": "What is the capital of France?",
    "stream": false,
    "options": {"num_predict": 20}
  }' | jq -r '.response'
END=$(date +%s%N)
ELAPSED=$((($END - $START) / 1000000))
echo "â±ï¸ TinyLlama ì¬í˜¸ì¶œ ì‘ë‹µ ì‹œê°„: ${ELAPSED}ms"
echo ""

# ìµœì¢… GPU ìƒíƒœ
echo "ğŸ“Š ìµœì¢… GPU ë©”ëª¨ë¦¬ ìƒíƒœ:"
nvidia-smi --query-gpu=memory.used,memory.free --format=csv
echo ""

# í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ í™•ì¸
echo "ğŸ“‹ í˜„ì¬ ë©”ëª¨ë¦¬ì— ìˆëŠ” ëª¨ë¸:"
docker exec ollama ollama list
echo ""

echo "âœ… Ollama ë©€í‹°ëª¨ë¸ ì„œë¹™ íŠ¹ì§•:"
echo "  â€¢ ë‹¨ì¼ API ì—”ë“œí¬ì¸íŠ¸ (11434 í¬íŠ¸)"
echo "  â€¢ ë™ì  ëª¨ë¸ ë¡œë”©/ì–¸ë¡œë”©"
echo "  â€¢ ìë™ ë©”ëª¨ë¦¬ ê´€ë¦¬"
echo "  â€¢ ëª¨ë¸ ê°„ ììœ ë¡œìš´ ì „í™˜"
echo "  â€¢ GGUF í¬ë§·ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ìš´ì˜"