# vLLM ë©€í‹°ëª¨ë¸ ì„œë¹™ ê°€ì´ë“œ (RTX 5090)

## ğŸ“‹ ê°œìš”
ì´ ì €ì¥ì†ŒëŠ” ë‹¨ì¼ NVIDIA RTX 5090 GPU (32GB VRAM)ì—ì„œ Docker ì»¨í…Œì´ë„ˆë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ LLM ëª¨ë¸ì„ ë™ì‹œì— ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì„ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ¯ í”„ë¡œì íŠ¸ ëª©í‘œ
- ë‹¨ì¼ GPUì—ì„œ ì—¬ëŸ¬ ëª¨ë¸ ë™ì‹œ ì„œë¹™
- ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì í™”
- ê° ëª¨ë¸ë³„ ë…ë¦½ì ì¸ ì—”ë“œí¬ì¸íŠ¸ ì œê³µ
- ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ê´€ë¦¬

## ğŸ“Š í…ŒìŠ¤íŠ¸ í™˜ê²½
- **GPU**: NVIDIA GeForce RTX 5090 (32GB VRAM)
- **CUDA**: 12.9
- **Driver**: 575.64.05
- **vLLM**: v0.10.1.1
- **Docker**: 24.0.7
- **OS**: Linux 6.15.10

## ğŸš€ ë°°í¬ëœ ëª¨ë¸

### 1. TinyLlama 1.1B
- **ìš©ë„**: ë¹ ë¥¸ ì‘ë‹µì´ í•„ìš”í•œ ê°„ë‹¨í•œ ì§ˆì˜
- **íŒŒë¼ë¯¸í„°**: 1.1B
- **ì‘ë‹µì‹œê°„**: í‰ê·  33ms
- **ë©”ëª¨ë¦¬ ì‚¬ìš©**: 5.3GB (ë©€í‹°ëª¨ë¸ í™˜ê²½)

### 2. Qwen 2.5-3B
- **ìš©ë„**: ë²”ìš© ì‘ì—…, ì½”ë“œ ìƒì„±, ì¼ë°˜ ëŒ€í™”
- **íŒŒë¼ë¯¸í„°**: 3B
- **ì‘ë‹µì‹œê°„**: í‰ê·  81ms
- **ë©”ëª¨ë¦¬ ì‚¬ìš©**: 8.4GB (ë©€í‹°ëª¨ë¸ í™˜ê²½)

### 3. Yi-6B
- **ìš©ë„**: ê³ í’ˆì§ˆ ì‘ë‹µì´ í•„ìš”í•œ ë³µì¡í•œ ì‘ì—…
- **íŒŒë¼ë¯¸í„°**: 6B
- **ì‘ë‹µì‹œê°„**: í‰ê·  96ms
- **ë©”ëª¨ë¦¬ ì‚¬ìš©**: 17.2GB (ë©€í‹°ëª¨ë¸ í™˜ê²½)

## ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìƒì„¸ ë¶„ì„

### ê°œë³„ ëª¨ë¸ í…ŒìŠ¤íŠ¸ (gpu-memory-utilization=0.9)
ê° ëª¨ë¸ì„ ë‹¨ë…ìœ¼ë¡œ ì‹¤í–‰í–ˆì„ ë•Œì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | ì´ë¡ ì  í¬ê¸° | ì‹¤ì œ ì‚¬ìš©ëŸ‰ | ë¹„ê³  |
|------|----------|-------------|-------------|------|
| TinyLlama 1.1B | 1.1B | 2.2GB | **29.5GB** | KV ìºì‹œê°€ ê°€ìš© ë©”ëª¨ë¦¬ ì „ì²´ ì‚¬ìš© |
| Qwen 2.5-3B | 3B | 6GB | **29.6GB** | ëª¨ë¸ í¬ê¸°ì™€ ë¬´ê´€í•˜ê²Œ ìµœëŒ€ í• ë‹¹ |
| Yi-6B | 6B | 12GB | **29.5GB** | gpu-memory-utilizationì´ ì „ì²´ ë©”ëª¨ë¦¬ì˜ 90% ì‚¬ìš© |

### ë©€í‹°ëª¨ë¸ ë™ì‹œ ì‹¤í–‰
ì„¸ ëª¨ë¸ì„ ë™ì‹œì— ì‹¤í–‰í–ˆì„ ë•Œ:

| ëª¨ë¸ | GPU ë©”ëª¨ë¦¬ | ë¹„ìœ¨ | ì‘ë‹µ ì‹œê°„ | ì„¤ì •ëœ utilization |
|------|------------|------|-----------|-------------------|
| TinyLlama | 5.3GB | 16% | 33ms | 0.15 |
| Qwen 2.5 | 8.4GB | 26% | 81ms | 0.25 |
| Yi-6B | 17.2GB | 53% | 96ms | 0.45 |
| **ì´í•©** | **31.5GB** | **96.6%** | - | 0.85 |

## âš ï¸ ì¤‘ìš” ë°œê²¬ì‚¬í•­

### 1. gpu-memory-utilization íŒŒë¼ë¯¸í„°ì˜ ì‹¤ì œ ë™ì‘

#### âŒ ì¼ë°˜ì ì¸ ì˜¤í•´
```yaml
gpu-memory-utilization: 0.5  # GPU ì „ì²´ ë©”ëª¨ë¦¬ì˜ 50% ì‚¬ìš©
```

#### âœ… ì‹¤ì œ ë™ì‘
```yaml
gpu-memory-utilization: 0.5  # ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ì˜ 50%ë¥¼ KV ìºì‹œì— í• ë‹¹
```

### 2. ë©”ëª¨ë¦¬ í• ë‹¹ ê³µì‹
```
ì‹¤ì œ ë©”ëª¨ë¦¬ = ëª¨ë¸ ê°€ì¤‘ì¹˜ + (ë‚¨ì€ ë©”ëª¨ë¦¬ Ã— gpu_utilization) + ì˜¤ë²„í—¤ë“œ
```

#### êµ¬ì²´ì  ì˜ˆì‹œ (Yi-6B)
```
1. ì „ì²´ GPU ë©”ëª¨ë¦¬: 32.6GB
2. ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ: 12GB (ë¨¼ì € í• ë‹¹)
3. ë‚¨ì€ ë©”ëª¨ë¦¬: 20.6GB
4. KV ìºì‹œ í• ë‹¹: 20.6GB Ã— 0.45 = 9.27GB
5. ì‹¤ì œ ì‚¬ìš©ëŸ‰: 12 + 9.27 + ì˜¤ë²„í—¤ë“œ â‰ˆ 22GB
```

### 3. ë©€í‹°ëª¨ë¸ ì‹¤í–‰ì˜ ë¬¸ì œì 
- ê° ì»¨í…Œì´ë„ˆê°€ ë…ë¦½ì ìœ¼ë¡œ GPU ë©”ëª¨ë¦¬ í™•ì¸
- ë‹¤ë¥¸ ì»¨í…Œì´ë„ˆì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì•Œ ìˆ˜ ì—†ìŒ
- ì¶©ëŒ ê°€ëŠ¥ì„±ì´ ë†’ì•„ ì„¸ì‹¬í•œ ì„¤ì • í•„ìš”

## ğŸ”§ ìƒì„¸ ì„¤ì • ê°€ì´ë“œ

### docker-compose-balanced.yml ì „ì²´ ì„¤ì •

```yaml
version: '3.8'

services:
  # TinyLlama - ë¹ ë¥¸ ì‘ë‹µìš© ê²½ëŸ‰ ëª¨ë¸
  tinyllama:
    image: vllm/vllm-openai:latest
    container_name: vllm-tinyllama
    ports:
      - "8001:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    command: [
      "--host", "0.0.0.0",
      "--model", "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
      "--served-model-name", "tinyllama",

      # ë©”ëª¨ë¦¬ ê´€ë ¨ ì„¤ì •
      "--gpu-memory-utilization", "0.15",  # GPU ë©”ëª¨ë¦¬ì˜ 15% ì‚¬ìš©
      "--max-model-len", "1024",           # ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ (KV ìºì‹œ í¬ê¸° ì œì–´)

      # ì„±ëŠ¥ ê´€ë ¨ ì„¤ì •
      "--max-num-seqs", "32",              # ë™ì‹œ ì²˜ë¦¬ ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤ ìˆ˜
      "--max-num-batched-tokens", "2048",  # ë°°ì¹˜ë‹¹ ìµœëŒ€ í† í° ìˆ˜

      # ìµœì í™” ì„¤ì •
      "--enforce-eager",                   # CUDA ì»´íŒŒì¼ ë¬¸ì œ ë°©ì§€
      "--disable-log-requests",            # ë¡œê·¸ ë¹„í™œì„±í™”ë¡œ ì„±ëŠ¥ í–¥ìƒ
      "--dtype", "float16",                # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ë°ì´í„° íƒ€ì…

      # ì¶”ê°€ ì˜µì…˜
      "--trust-remote-code",               # ëª¨ë¸ì˜ ì»¤ìŠ¤í…€ ì½”ë“œ ì‹¤í–‰ í—ˆìš©
      "--enable-prefix-caching"            # í”„ë¦¬í”½ìŠ¤ ìºì‹± í™œì„±í™”
    ]
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Qwen - ë²”ìš© ì¤‘í˜• ëª¨ë¸
  qwen:
    image: vllm/vllm-openai:latest
    container_name: vllm-qwen
    ports:
      - "8002:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    command: [
      "--host", "0.0.0.0",
      "--model", "Qwen/Qwen2.5-3B-Instruct",
      "--served-model-name", "qwen",

      # ë©”ëª¨ë¦¬ ê´€ë ¨ ì„¤ì •
      "--gpu-memory-utilization", "0.25",  # GPU ë©”ëª¨ë¦¬ì˜ 25% ì‚¬ìš©
      "--max-model-len", "1536",           # ì¤‘ê°„ í¬ê¸° ì»¨í…ìŠ¤íŠ¸

      # ì„±ëŠ¥ ê´€ë ¨ ì„¤ì •
      "--max-num-seqs", "24",              # ì¤‘ê°„ ìˆ˜ì¤€ì˜ ë™ì‹œ ì²˜ë¦¬
      "--max-num-batched-tokens", "3072",

      # ìµœì í™” ì„¤ì •
      "--enforce-eager",
      "--disable-log-requests",
      "--dtype", "float16",

      # Qwen íŠ¹í™” ì„¤ì •
      "--trust-remote-code",               # Qwen ëª¨ë¸ í•„ìˆ˜
      "--tensor-parallel-size", "1"        # ë‹¨ì¼ GPU ì‚¬ìš©
    ]
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    depends_on:
      - tinyllama  # TinyLlama ì‹œì‘ í›„ ì‹¤í–‰
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Yi - ê³ í’ˆì§ˆ ëŒ€í˜• ëª¨ë¸
  yi:
    image: vllm/vllm-openai:latest
    container_name: vllm-yi
    ports:
      - "8003:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    command: [
      "--host", "0.0.0.0",
      "--model", "01-ai/Yi-6B-Chat",
      "--served-model-name", "yi",

      # ë©”ëª¨ë¦¬ ê´€ë ¨ ì„¤ì •
      "--gpu-memory-utilization", "0.45",  # GPU ë©”ëª¨ë¦¬ì˜ 45% ì‚¬ìš©
      "--max-model-len", "2048",           # ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸

      # ì„±ëŠ¥ ê´€ë ¨ ì„¤ì •
      "--max-num-seqs", "16",              # í’ˆì§ˆ ìš°ì„ , ì ì€ ë™ì‹œ ì²˜ë¦¬
      "--max-num-batched-tokens", "4096",

      # ìµœì í™” ì„¤ì •
      "--enforce-eager",
      "--disable-log-requests",
      "--dtype", "float16",

      # ì¶”ê°€ ìµœì í™”
      "--enable-prefix-caching",
      "--use-v2-block-manager"            # ê°œì„ ëœ ë¸”ë¡ ê´€ë¦¬ì
    ]
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    depends_on:
      - qwen  # Qwen ì‹œì‘ í›„ ì‹¤í–‰
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Nginx API Gateway (ì„ íƒì‚¬í•­)
  api-gateway:
    image: nginx:alpine
    container_name: vllm-gateway
    ports:
      - "8000:80"
    volumes:
      - ./nginx-gateway.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - tinyllama
      - qwen
      - yi
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # GPU ëª¨ë‹ˆí„°ë§ (ì„ íƒì‚¬í•­)
  gpu-monitor:
    image: nvidia/dcgm-exporter:latest
    container_name: gpu-monitor
    ports:
      - "9400:9400"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

networks:
  default:
    driver: bridge
```

### ì£¼ìš” íŒŒë¼ë¯¸í„° ì„¤ëª…

#### ë©”ëª¨ë¦¬ ê´€ë ¨ íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ì„¤ëª… | ê¶Œì¥ê°’ ë²”ìœ„ |
|---------|------|------------|
| `--gpu-memory-utilization` | KV ìºì‹œì— í• ë‹¹í•  ë©”ëª¨ë¦¬ ë¹„ìœ¨ | 0.1 ~ 0.9 |
| `--max-model-len` | ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (í† í°) | 512 ~ 4096 |
| `--swap-space` | ìŠ¤ì™‘ ê³µê°„ í¬ê¸° (GB) | 4 ~ 16 |
| `--cpu-offload-gb` | CPUë¡œ ì˜¤í”„ë¡œë“œí•  í¬ê¸° | 0 ~ 8 |

#### ì„±ëŠ¥ ê´€ë ¨ íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ì„¤ëª… | ê¶Œì¥ê°’ ë²”ìœ„ |
|---------|------|------------|
| `--max-num-seqs` | ë™ì‹œ ì²˜ë¦¬ ì‹œí€€ìŠ¤ ìˆ˜ | 1 ~ 256 |
| `--max-num-batched-tokens` | ë°°ì¹˜ë‹¹ ìµœëŒ€ í† í° | 512 ~ 8192 |
| `--max-parallel-loading-workers` | ë³‘ë ¬ ë¡œë”© ì›Œì»¤ ìˆ˜ | 1 ~ 4 |
| `--block-size` | KV ìºì‹œ ë¸”ë¡ í¬ê¸° | 8, 16, 32 |

#### ìµœì í™” íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ì„¤ëª… | ì‚¬ìš© ì‹œê¸° |
|---------|------|-----------|
| `--enforce-eager` | Eager ëª¨ë“œ ê°•ì œ | RTX 5090 í•„ìˆ˜ |
| `--disable-log-requests` | ìš”ì²­ ë¡œê·¸ ë¹„í™œì„±í™” | í”„ë¡œë•ì…˜ |
| `--enable-prefix-caching` | í”„ë¦¬í”½ìŠ¤ ìºì‹± | ë°˜ë³µ ì¿¼ë¦¬ ë§ì„ ë•Œ |
| `--use-v2-block-manager` | ê°œì„ ëœ ë¸”ë¡ ê´€ë¦¬ì | ë©”ëª¨ë¦¬ íš¨ìœ¨ í•„ìš” ì‹œ |

## ğŸ“ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 1. ì‹œì‘ ìˆœì„œ ìµœì í™”
```bash
# ì˜¬ë°”ë¥¸ ìˆœì„œ: ì‘ì€ ëª¨ë¸ â†’ í° ëª¨ë¸
docker compose up -d tinyllama
sleep 30
docker compose up -d qwen
sleep 30
docker compose up -d yi
```

### 2. ë©”ëª¨ë¦¬ í• ë‹¹ ì „ëµ

#### ë³´ìˆ˜ì  ì„¤ì • (ì•ˆì •ì„± ìš°ì„ )
```yaml
TinyLlama: --gpu-memory-utilization 0.10
Qwen:      --gpu-memory-utilization 0.20
Yi:        --gpu-memory-utilization 0.40
ì—¬ìœ  ê³µê°„: 30%
```

#### ê· í˜• ì„¤ì • (ì¶”ì²œ)
```yaml
TinyLlama: --gpu-memory-utilization 0.15
Qwen:      --gpu-memory-utilization 0.25
Yi:        --gpu-memory-utilization 0.45
ì—¬ìœ  ê³µê°„: 15%
```

#### ê³µê²©ì  ì„¤ì • (ì„±ëŠ¥ ìš°ì„ )
```yaml
TinyLlama: --gpu-memory-utilization 0.18
Qwen:      --gpu-memory-utilization 0.28
Yi:        --gpu-memory-utilization 0.50
ì—¬ìœ  ê³µê°„: 4%
```

### 3. ëª¨ë‹ˆí„°ë§ ëª…ë ¹ì–´

```bash
# ì‹¤ì‹œê°„ GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# í”„ë¡œì„¸ìŠ¤ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
nvidia-smi --query-compute-apps=pid,process_name,used_gpu_memory --format=csv

# ì»¨í…Œì´ë„ˆ ë¡œê·¸ í™•ì¸
docker logs vllm-tinyllama --tail 50 -f
docker logs vllm-qwen --tail 50 -f
docker logs vllm-yi --tail 50 -f

# í—¬ìŠ¤ ì²´í¬
curl http://localhost:8001/health
curl http://localhost:8002/health
curl http://localhost:8003/health

# ëª¨ë¸ ì •ë³´
curl http://localhost:8001/v1/models
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

### 1. ì‚¬ì „ ìš”êµ¬ì‚¬í•­ ì„¤ì¹˜

```bash
# NVIDIA Driver ì„¤ì¹˜ (CUDA 12.8+ ì§€ì›)
sudo apt-get update
sudo apt-get install -y nvidia-driver-545

# Docker ì„¤ì¹˜
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# NVIDIA Container Toolkit ì„¤ì¹˜
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

### 2. ì €ì¥ì†Œ í´ë¡  ë° ì„¤ì •

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/Cloud-Linuxer/multi-model-server.git
cd multi-model-server

# HuggingFace ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p ~/.cache/huggingface

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì„ íƒì‚¬í•­)
export HUGGING_FACE_HUB_TOKEN="your_token_here"
```

### 3. ëª¨ë¸ ì‹¤í–‰

```bash
# ëª¨ë“  ëª¨ë¸ ì‹œì‘
docker compose -f docker-compose-balanced.yml up -d

# ìƒíƒœ í™•ì¸
docker ps | grep vllm

# ë¡œê·¸ í™•ì¸ (ë¡œë”© ì§„í–‰ìƒí™©)
docker compose -f docker-compose-balanced.yml logs -f
```

### 4. í…ŒìŠ¤íŠ¸

```bash
# TinyLlama í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8001/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tinyllama",
    "prompt": "ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ë‚ ì”¨ê°€",
    "max_tokens": 50,
    "temperature": 0.7
  }'

# Qwen í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8002/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen",
    "prompt": "def fibonacci(n):",
    "max_tokens": 100,
    "temperature": 0.1
  }'

# Yi í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8003/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "yi",
    "prompt": "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”:",
    "max_tokens": 200,
    "temperature": 0.5
  }'
```

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. CUDA ì˜¤ë¥˜: "no kernel image is available"
```bash
# í•´ê²°ì±…: enforce-eager í”Œë˜ê·¸ ì¶”ê°€
--enforce-eager
```

### 2. ë©”ëª¨ë¦¬ ë¶€ì¡±: "No available memory for the cache blocks"
```bash
# í•´ê²°ì±… 1: gpu-memory-utilization ì¤„ì´ê¸°
--gpu-memory-utilization 0.3  # 0.5ì—ì„œ 0.3ìœ¼ë¡œ

# í•´ê²°ì±… 2: max-model-len ì¤„ì´ê¸°
--max-model-len 1024  # 2048ì—ì„œ 1024ë¡œ

# í•´ê²°ì±… 3: max-num-seqs ì¤„ì´ê¸°
--max-num-seqs 16  # 32ì—ì„œ 16ìœ¼ë¡œ
```

### 3. ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: "401 Client Error"
```bash
# í•´ê²°ì±…: í† í°ì´ í•„ìš”í•œ ëª¨ë¸ì˜ ê²½ìš°
export HUGGING_FACE_HUB_TOKEN="your_token"
docker compose up -d
```

### 4. ëŠë¦° ì‘ë‹µ ì†ë„
```bash
# í•´ê²°ì±… 1: dtypeì„ float16ìœ¼ë¡œ ê³ ì •
--dtype float16

# í•´ê²°ì±… 2: ë°°ì¹˜ í¬ê¸° ìµœì í™”
--max-num-batched-tokens 2048

# í•´ê²°ì±… 3: prefix caching í™œì„±í™”
--enable-prefix-caching
```

### 5. ì»¨í…Œì´ë„ˆê°€ ê³„ì† ì¬ì‹œì‘
```bash
# ë¡œê·¸ í™•ì¸
docker logs vllm-yi --tail 100

# í”í•œ ì›ì¸ë“¤:
# - ë©”ëª¨ë¦¬ ë¶€ì¡±: gpu-memory-utilization ì¤„ì´ê¸°
# - í¬íŠ¸ ì¶©ëŒ: ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
# - ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: ë„¤íŠ¸ì›Œí¬ í™•ì¸
```

## ğŸ“ˆ ì„±ëŠ¥ ë©”íŠ¸ë¦­

### í…ŒìŠ¤íŠ¸ ê²°ê³¼ (RTX 5090)

| ë©”íŠ¸ë¦­ | TinyLlama | Qwen 2.5 | Yi-6B |
|--------|-----------|----------|-------|
| ì²« í† í°ê¹Œì§€ ì‹œê°„ | 25ms | 65ms | 85ms |
| í‰ê·  ì‘ë‹µ ì‹œê°„ | 33ms | 81ms | 96ms |
| ì²˜ë¦¬ëŸ‰ (í† í°/ì´ˆ) | 450 | 280 | 210 |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | 5.3GB | 8.4GB | 17.2GB |
| ìµœëŒ€ ë™ì‹œ ìš”ì²­ | 32 | 24 | 16 |

### ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼

```bash
# ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸ (ê° ëª¨ë¸ 10ê°œ ìš”ì²­)
ab -n 100 -c 10 http://localhost:8001/v1/completions
# TinyLlama: 98% ì„±ê³µ, í‰ê·  120ms

ab -n 100 -c 10 http://localhost:8002/v1/completions
# Qwen: 95% ì„±ê³µ, í‰ê·  250ms

ab -n 100 -c 10 http://localhost:8003/v1/completions
# Yi: 92% ì„±ê³µ, í‰ê·  380ms
```

## ğŸ”¬ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­

### vLLM ë©”ëª¨ë¦¬ ê´€ë¦¬ ë©”ì»¤ë‹ˆì¦˜

#### PagedAttention
- KV ìºì‹œë¥¼ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ê´€ë¦¬
- ë™ì  ë©”ëª¨ë¦¬ í• ë‹¹ ë° í•´ì œ
- ë©”ëª¨ë¦¬ ë‹¨í¸í™” ìµœì†Œí™”

#### ë©”ëª¨ë¦¬ ê³„ì‚° ê³µì‹
```python
# ëª¨ë¸ ê°€ì¤‘ì¹˜ ë©”ëª¨ë¦¬
model_memory = num_parameters * bytes_per_parameter

# KV ìºì‹œ ë©”ëª¨ë¦¬
kv_cache_memory = (
    num_layers * 2 * max_seq_len *
    hidden_size * batch_size * bytes_per_element
)

# í™œì„±í™” ë©”ëª¨ë¦¬
activation_memory = hidden_size * max_seq_len * batch_size * 4

# ì´ ë©”ëª¨ë¦¬
total_memory = model_memory + kv_cache_memory + activation_memory + overhead
```

### RTX 5090 íŠ¹í™” ìµœì í™”

#### CUDA ì„¤ì •
```bash
# í™˜ê²½ ë³€ìˆ˜
export CUDA_VISIBLE_DEVICES=0
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export CUDA_LAUNCH_BLOCKING=0
```

#### ì»´íŒŒì¼ ìµœì í™”
```yaml
# vLLM í”Œë˜ê·¸
--enforce-eager              # JIT ì»´íŒŒì¼ ë¹„í™œì„±í™”
--disable-custom-all-reduce  # ì»¤ìŠ¤í…€ í†µì‹  ë¹„í™œì„±í™”
--tensor-parallel-size 1     # ë‹¨ì¼ GPU
```

## ğŸ”„ SGLang ëŒ€ì•ˆ í‰ê°€

### SGLang í…ŒìŠ¤íŠ¸ ê²°ê³¼ (2025ë…„ 9ì›” 18ì¼)

#### âŒ RTX 5090 í˜¸í™˜ì„± ë¬¸ì œ
SGLangì€ í˜„ì¬ RTX 5090ì—ì„œ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤:

```
NVIDIA GeForce RTX 5090 with CUDA capability sm_120 is not compatible
PyTorch supports: sm_50 sm_60 sm_70 sm_75 sm_80 sm_86 sm_90
í•„ìš”í•œ ì§€ì›: sm_120 (CUDA 12.0)
```

#### í…ŒìŠ¤íŠ¸ ì‹œë„ ë‚´ì—­
| ì‹œë„ | ë°©ë²• | ê²°ê³¼ |
|------|------|------|
| ê¸°ë³¸ ì‹¤í–‰ | `lmsysorg/sglang:latest` | CUDA ì»¤ë„ ì˜¤ë¥˜ |
| í™˜ê²½ ë³€ìˆ˜ | `CUDA_LAUNCH_BLOCKING=1` | ë™ì¼í•œ ì˜¤ë¥˜ |
| ì•„í‚¤í…ì²˜ ì§€ì • | `TORCH_CUDA_ARCH_LIST="8.6;9.0"` | sm_120 ë¯¸ì§€ì› |
| í”Œë˜ê·¸ ì¡°ì • | `--disable-cuda-graph` ì œê±° | ê·¼ë³¸ì  í˜¸í™˜ì„± ë¬¸ì œ |

#### SGLang vs vLLM ë¹„êµí‘œ

| íŠ¹ì„± | vLLM (RTX 5090) | SGLang (RTX 5090) | SGLang (ì´ë¡ ì ) |
|------|-----------------|-------------------|-----------------|
| **RTX 5090 ì§€ì›** | âœ… ì‘ë™ | âŒ ë¯¸ì§€ì› | - |
| **ë©”ëª¨ë¦¬ ê´€ë¦¬** | PagedAttention | - | RadixAttention |
| **í”„ë¦¬í”½ìŠ¤ ìºì‹±** | ì œí•œì  | - | ìµœëŒ€ 10x ì†ë„ í–¥ìƒ |
| **êµ¬ì¡°í™”ëœ ìƒì„±** | ì§€ì› | - | ìš°ìˆ˜í•œ ì§€ì› |
| **ì‹¤ì¸¡ ì„±ëŠ¥** | 33-96ms | ì¸¡ì • ë¶ˆê°€ | - |
| **ì•ˆì •ì„±** | ë†’ìŒ | - | ê°œë°œ ì¤‘ |

### ê¶Œì¥ì‚¬í•­

#### RTX 5090 ì‚¬ìš©ì
- **í˜„ì¬**: vLLMì´ ìœ ì¼í•œ ì„ íƒ
- **í–¥í›„**: SGLangì˜ PyTorch 2.5+ ì—…ë°ì´íŠ¸ ëŒ€ê¸°
- **ì„ì‹œì±…**: RTX 4090ì—ì„œ SGLang í…ŒìŠ¤íŠ¸ ê³ ë ¤

#### RTX 4090 ì´í•˜ ì‚¬ìš©ì
SGLang ê³ ë ¤ ê°€ëŠ¥í•œ ê²½ìš°:
- ë°˜ë³µì ì¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
- API ì„œë¹™ì´ ì£¼ ëª©ì 
- êµ¬ì¡°í™”ëœ ì¶œë ¥ í•„ìš”
- ë‚®ì€ ì§€ì—°ì‹œê°„ ìš”êµ¬

## ğŸ“š í•™ìŠµëœ êµí›ˆ

### 1. gpu-memory-utilizationì˜ ì‹¤ì œ ì˜ë¯¸
- **ì ˆëŒ€ê°’ì´ ì•„ë‹Œ ìƒëŒ€ê°’**: ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ì˜ ë¹„ìœ¨
- **ëª¨ë¸ í¬ê¸° ë¬´ê´€**: ì‘ì€ ëª¨ë¸ë„ í° KV ìºì‹œ í• ë‹¹ ê°€ëŠ¥
- **ë™ì  ì¡°ì • í•„ìš”**: ì‹¤ì œ ì‚¬ìš© íŒ¨í„´ì— ë”°ë¼ ì¡°ì •

### 2. ë©€í‹°ëª¨ë¸ ì„œë¹™ì˜ ë³µì¡ì„±
- **ë…ë¦½ì  ë©”ëª¨ë¦¬ ê´€ë¦¬**: ê° ì»¨í…Œì´ë„ˆê°€ ë…ë¦½ì ìœ¼ë¡œ ì‘ë™
- **ìˆ˜ë™ ìµœì í™” í•„ìˆ˜**: ìë™í™”ë³´ë‹¤ ìˆ˜ë™ íŠœë‹ì´ íš¨ê³¼ì 
- **ì§€ì†ì  ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ ì¡°ì • í•„ìš”

### 3. ì‹¤ì œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- **ì´ë¡ ê°’ì˜ 2-2.5ë°°**: ëª¨ë¸ ê°€ì¤‘ì¹˜ë§Œìœ¼ë¡œ ê³„ì‚° ë¶ˆê°€
- **ì˜¤ë²„í—¤ë“œ ê³ ë ¤**: CUDA ì»¤ë„, ë²„í¼, í˜ì´ì§€ í…Œì´ë¸” ë“±
- **ì—¬ìœ  ê³µê°„ í•„ìˆ˜**: ìµœì†Œ 5-10% ì—¬ìœ  í™•ë³´

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

ë‹¤ë¥¸ GPU í™˜ê²½ì—ì„œì˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë‚˜ ê°œì„ ì‚¬í•­ì´ ìˆë‹¤ë©´ PRì„ ë³´ë‚´ì£¼ì„¸ìš”!

### í…ŒìŠ¤íŠ¸ í™˜ê²½ ì¶”ê°€ ë°©ë²•
1. `configs/` ë””ë ‰í† ë¦¬ì— ìƒˆ ì„¤ì • íŒŒì¼ ì¶”ê°€
2. `scripts/` ë””ë ‰í† ë¦¬ì— í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì¶”ê°€
3. READMEì— ê²°ê³¼ ë¬¸ì„œí™”
4. PR ì œì¶œ

## ğŸ“„ ë¼ì´ì„¼ìŠ¤
MIT License

## ğŸ”— ì°¸ê³  ìë£Œ
- [vLLM ê³µì‹ ë¬¸ì„œ](https://docs.vllm.ai)
- [NVIDIA Container Toolkit](https://github.com/NVIDIA/nvidia-docker)
- [Docker Compose ë¬¸ì„œ](https://docs.docker.com/compose/)
- [HuggingFace Models](https://huggingface.co/models)

## ğŸ“ ë¬¸ì˜
- Issue: [GitHub Issues](https://github.com/Cloud-Linuxer/multi-model-server/issues)
- Email: cloud.linuxer@example.com

---
*ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: 2025ë…„ 9ì›” 18ì¼*
*í…ŒìŠ¤íŠ¸ í™˜ê²½: NVIDIA GeForce RTX 5090 (32GB) | CUDA 12.9 | vLLM v0.10.1.1*