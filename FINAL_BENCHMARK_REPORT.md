# 🔬 SGLang vs vLLM 최종 성능 비교 보고서

**생성일**: 2025-09-18 15:56
**GPU**: NVIDIA RTX 5090 (32GB)
**테스트 모델**: TinyLlama 1.1B
**테스트 언어**: 영어, 중국어, 한국어
**테스트 규모**: 각 프레임워크당 300개 테스트 (언어당 100개)

## 📊 Executive Summary

### 🏆 최종 승자: vLLM
vLLM이 동일한 조건에서 SGLang보다 **4.8배 높은 처리량**과 **33.6% 낮은 지연시간**을 달성했습니다.

### 전체 성능 비교

| 메트릭 | SGLang | vLLM | 개선율 |
|--------|--------|------|--------|
| **성공률** | 100% (300/300) | 99.7% (299/300) | -0.3% |
| **평균 지연시간** | 393.94ms | **261.73ms** | ✅ 33.6% 개선 |
| **P50 지연시간** | 526.71ms | **261.22ms** | ✅ 50.4% 개선 |
| **P95 지연시간** | 538.42ms | **265.33ms** | ✅ 50.7% 개선 |
| **평균 처리량** | 69.38 tok/s | **332.03 tok/s** | ✅ 378.6% 개선 |
| **지연시간 일관성** | P95-P50: 11.7ms | **P95-P50: 4.1ms** | ✅ 65% 더 일관됨 |

## 🌍 언어별 성능 분석

### 영어 (English)

| 메트릭 | SGLang | vLLM | 승자 |
|--------|--------|------|------|
| 평균 지연시간 | 472.10ms | **404.79ms** | vLLM ✅ |
| 평균 처리량 | 145.79 tok/s | **369.62 tok/s** | vLLM ✅ |
| 평균 토큰 생성 | - | 94.2 | - |

### 중국어 (Chinese)

| 메트릭 | SGLang | vLLM | 승자 |
|--------|--------|------|------|
| 평균 지연시간 | 456.54ms | **222.83ms** | vLLM ✅ |
| 평균 처리량 | 33.38 tok/s | **343.21 tok/s** | vLLM ✅ |
| 평균 토큰 생성 | - | 84.6 | - |

### 한국어 (Korean)

| 메트릭 | SGLang | vLLM | 승자 |
|--------|--------|------|------|
| 평균 지연시간 | 253.17ms | **159.00ms** | vLLM ✅ |
| 평균 처리량 | 28.96 tok/s | **283.64 tok/s** | vLLM ✅ |
| 평균 토큰 생성 | - | 59.5 | - |

## 📈 상세 성능 메트릭

### vLLM 지연시간 분포
```
최소값: 7.51ms
P25: ~260ms
P50: 261.22ms
P75: 262.86ms
P90: 264.02ms
P95: 265.33ms
P99: 267.09ms
최대값: 15740.88ms (outlier)
```

### SGLang 지연시간 분포
```
최소값: ~250ms
P25: ~400ms
P50: 526.71ms
P75: ~530ms
P95: 538.42ms
최대값: ~540ms
```

## 🎯 주요 발견사항

### 1. 성능 우위
- **vLLM이 모든 언어에서 우수**: 영어, 중국어, 한국어 모두에서 vLLM이 더 빠름
- **처리량 차이**: vLLM이 평균 4.8배 높은 처리량 달성
- **일관성**: vLLM의 P95-P50 차이가 4.1ms로 SGLang의 11.7ms보다 훨씬 일관됨

### 2. 언어별 특성
- **영어**: 가장 높은 처리량 (vLLM: 369.62 tok/s)
- **중국어**: 가장 큰 성능 개선 (10배 처리량 증가)
- **한국어**: 가장 빠른 응답 시간 (vLLM: 159ms)

### 3. RTX 5090 호환성 (수정됨)
- **SGLang**: 커스텀 빌드 필요, 많은 최적화 비활성화
- **vLLM**: ✅ 최신 버전 정상 작동 (올바른 설정 시)

## 💡 권장사항

### RTX 5090 사용자

**단일 모델 서비스**: **vLLM 강력 추천**
- 4.8배 높은 처리량
- 33.6% 낮은 지연시간
- 더 일관된 성능
- 표준 Docker 이미지 사용 가능

**멀티모델 서비스**: **vLLM 사용**
- vLLM은 멀티모델 동시 실행 지원
- SGLang은 단일 모델만 가능

### 프로덕션 환경 권장 설정

#### vLLM 권장 설정
```bash
docker run -d \
  --runtime nvidia \
  --gpus all \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --gpu-memory-utilization 0.15 \
  --max-model-len 2048 \
  --dtype float16 \
  --enforce-eager
```

## 🏆 최종 결론

### vLLM이 명확한 승자

**성능 측면**:
- ✅ 4.8배 높은 처리량 (332 vs 69 tok/s)
- ✅ 33.6% 낮은 지연시간 (262 vs 394ms)
- ✅ 더 일관된 성능 (P95-P50: 4ms vs 12ms)

**운영 측면**:
- ✅ 멀티모델 지원
- ✅ 표준 Docker 이미지
- ✅ 더 나은 RTX 5090 호환성

**유일한 단점**:
- 성공률 0.3% 낮음 (99.7% vs 100%)

## 📝 테스트 데이터

### CSV 파일 목록
- `vllm_benchmark_20250918_155524.csv` - vLLM 300개 테스트 결과
- `quick_benchmark_20250918_154651.csv` - SGLang 300개 테스트 결과
- 총 600개 테스트 데이터 포인트

### 테스트 프롬프트
- **영어**: "Sing a beautiful song about spring with blooming flowers."
- **중국어**: "唱一首关于春天百花盛开之美的歌曲。"
- **한국어**: "봄에 피어나는 꽃들의 아름다움을 노래해주세요."

### 테스트 환경
- **하드웨어**: NVIDIA RTX 5090 (32GB VRAM)
- **소프트웨어**:
  - SGLang: blackwell-final-v2 (커스텀 빌드)
  - vLLM: latest (표준 Docker 이미지)
- **테스트 조건**:
  - 동일한 메모리 할당 (약 15%)
  - 동일한 max_tokens (100)
  - 동일한 온도 (0.7)

---

*최종 벤치마크 완료: 2025-09-18 15:56*
*총 테스트: 600개 (각 프레임워크 300개)*
*데이터 신뢰도: 99.8% (599/600 성공)*