#!/usr/bin/env python3
"""
SGLang í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
SGLangì˜ ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import json
import time
import requests
from typing import List, Dict

# SGLang ì—”ë“œí¬ì¸íŠ¸
ENDPOINTS = {
    "tinyllama": "http://localhost:30001",
    "qwen": "http://localhost:30002",
    "yi": "http://localhost:30003"
}

def test_basic_completion(endpoint: str, model_name: str):
    """ê¸°ë³¸ í…ìŠ¤íŠ¸ ì™„ì„± í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ§ª {model_name} ê¸°ë³¸ ì™„ì„± í…ŒìŠ¤íŠ¸")

    payload = {
        "text": "The future of AI is",
        "max_new_tokens": 50,
        "temperature": 0.7
    }

    start = time.time()
    response = requests.post(f"{endpoint}/generate", json=payload)
    latency = (time.time() - start) * 1000

    if response.status_code == 200:
        result = response.json()
        print(f"âœ… ì‘ë‹µ: {result.get('text', '')[:100]}...")
        print(f"â±ï¸ ì§€ì—°ì‹œê°„: {latency:.2f}ms")
    else:
        print(f"âŒ ì˜¤ë¥˜: {response.status_code}")

def test_structured_generation(endpoint: str, model_name: str):
    """êµ¬ì¡°í™”ëœ ìƒì„± í…ŒìŠ¤íŠ¸ (SGLang íŠ¹í™” ê¸°ëŠ¥)"""
    print(f"\nğŸ§ª {model_name} êµ¬ì¡°í™”ëœ ìƒì„± í…ŒìŠ¤íŠ¸")

    # JSON ìŠ¤í‚¤ë§ˆ ê°•ì œ
    payload = {
        "text": "Generate a person profile:",
        "max_new_tokens": 100,
        "temperature": 0.7,
        "regex": r'\{"name": "[^"]+", "age": \d+, "city": "[^"]+"\}'
    }

    start = time.time()
    response = requests.post(f"{endpoint}/generate", json=payload)
    latency = (time.time() - start) * 1000

    if response.status_code == 200:
        result = response.json()
        print(f"âœ… êµ¬ì¡°í™”ëœ ì‘ë‹µ: {result.get('text', '')}")
        print(f"â±ï¸ ì§€ì—°ì‹œê°„: {latency:.2f}ms")
    else:
        print(f"âŒ ì˜¤ë¥˜: {response.status_code}")

def test_batch_generation(endpoint: str, model_name: str):
    """ë°°ì¹˜ ìƒì„± í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ§ª {model_name} ë°°ì¹˜ ìƒì„± í…ŒìŠ¤íŠ¸")

    prompts = [
        "What is machine learning?",
        "Explain quantum computing",
        "Define artificial intelligence"
    ]

    payload = {
        "text": prompts,
        "max_new_tokens": 30,
        "temperature": 0.5,
        "parallel_sample_num": len(prompts)
    }

    start = time.time()
    response = requests.post(f"{endpoint}/generate", json=payload)
    latency = (time.time() - start) * 1000

    if response.status_code == 200:
        results = response.json()
        print(f"âœ… ë°°ì¹˜ í¬ê¸°: {len(prompts)}")
        print(f"â±ï¸ ì´ ì§€ì—°ì‹œê°„: {latency:.2f}ms")
        print(f"â±ï¸ í‰ê·  ì§€ì—°ì‹œê°„: {latency/len(prompts):.2f}ms/ìš”ì²­")
    else:
        print(f"âŒ ì˜¤ë¥˜: {response.status_code}")

def test_radix_attention_cache(endpoint: str, model_name: str):
    """RadixAttention ìºì‹œ íš¨ê³¼ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ§ª {model_name} RadixAttention ìºì‹œ í…ŒìŠ¤íŠ¸")

    # ê°™ì€ í”„ë¦¬í”½ìŠ¤ë¡œ ì—¬ëŸ¬ ìš”ì²­
    prefix = "You are a helpful AI assistant. Please answer the following question: "
    questions = [
        "What is the capital of France?",
        "What is the capital of Germany?",
        "What is the capital of Italy?"
    ]

    latencies = []
    for i, question in enumerate(questions):
        payload = {
            "text": prefix + question,
            "max_new_tokens": 20,
            "temperature": 0.1
        }

        start = time.time()
        response = requests.post(f"{endpoint}/generate", json=payload)
        latency = (time.time() - start) * 1000
        latencies.append(latency)

        if response.status_code == 200:
            print(f"  ìš”ì²­ {i+1}: {latency:.2f}ms")

    if latencies:
        print(f"âœ… ì²« ìš”ì²­: {latencies[0]:.2f}ms")
        print(f"âœ… ìºì‹œëœ ìš”ì²­ í‰ê· : {sum(latencies[1:])/len(latencies[1:]):.2f}ms")
        speedup = latencies[0] / (sum(latencies[1:])/len(latencies[1:]))
        print(f"ğŸš€ ì†ë„ í–¥ìƒ: {speedup:.2f}x")

def compare_with_vllm():
    """vLLMê³¼ ì„±ëŠ¥ ë¹„êµ"""
    print("\n" + "="*50)
    print("ğŸ“Š SGLang vs vLLM ì„±ëŠ¥ ë¹„êµ")
    print("="*50)

    # SGLang íŠ¹ì§•
    print("\nâœ¨ SGLang ì¥ì :")
    print("  - RadixAttention: í”„ë¦¬í”½ìŠ¤ ìºì‹±ìœ¼ë¡œ ìµœëŒ€ 10x ì†ë„ í–¥ìƒ")
    print("  - êµ¬ì¡°í™”ëœ ìƒì„±: JSON/ì½”ë“œ ìƒì„± ë³´ì¥")
    print("  - ë” ë¹ ë¥¸ ì²« í† í° ì‹œê°„ (TTFT)")
    print("  - íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬")

    print("\nğŸ“‰ SGLang ë‹¨ì :")
    print("  - ë” ì ì€ ëª¨ë¸ ì§€ì›")
    print("  - ëœ ì„±ìˆ™í•œ ìƒíƒœê³„")
    print("  - ì¼ë¶€ ê¸°ëŠ¥ ì œí•œì ")

    print("\nğŸ¯ ì‚¬ìš© ì¶”ì²œ ì‹œë‚˜ë¦¬ì˜¤:")
    print("  SGLang: API ì„œë¹™, êµ¬ì¡°í™”ëœ ì¶œë ¥, ë°˜ë³µì  í”„ë¡¬í”„íŠ¸")
    print("  vLLM: ë‹¤ì–‘í•œ ëª¨ë¸, ì•ˆì •ì„± ìš°ì„ , í”„ë¡œë•ì…˜")

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ SGLang ë©€í‹°ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")

    for model_name, endpoint in ENDPOINTS.items():
        print(f"\n{'='*50}")
        print(f"ğŸ“¦ {model_name.upper()} ëª¨ë¸ í…ŒìŠ¤íŠ¸")
        print('='*50)

        try:
            # í—¬ìŠ¤ ì²´í¬
            response = requests.get(f"{endpoint}/health", timeout=2)
            if response.status_code != 200:
                print(f"â³ {model_name} ì•„ì§ ì¤€ë¹„ ì•ˆë¨")
                continue

            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            test_basic_completion(endpoint, model_name)
            test_structured_generation(endpoint, model_name)
            test_batch_generation(endpoint, model_name)
            test_radix_attention_cache(endpoint, model_name)

        except requests.exceptions.RequestException as e:
            print(f"âŒ {model_name} ì—°ê²° ì‹¤íŒ¨: {e}")

    # ë¹„êµ ë¶„ì„
    compare_with_vllm()

if __name__ == "__main__":
    main()