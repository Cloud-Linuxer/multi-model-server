#!/bin/bash

echo "ğŸ”¬ vLLM ë©”ëª¨ë¦¬ ì œì–´ ì˜µì…˜ í…ŒìŠ¤íŠ¸"
echo ""
echo "vLLMì˜ ë©”ëª¨ë¦¬ ì œì–´ íŒŒë¼ë¯¸í„°ë“¤:"
echo "1. --gpu-memory-utilization: KV ìºì‹œìš© ê°€ìš© ë©”ëª¨ë¦¬ ë¹„ìœ¨ (0.1~0.9)"
echo "2. --max-model-len: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì§ì ‘ ì˜í–¥)"
echo "3. --enforce-eager: Eager ëª¨ë“œ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ)"
echo "4. --kv-cache-dtype: KV ìºì‹œ ë°ì´í„° íƒ€ì… (fp8 ì‚¬ìš© ì‹œ ë©”ëª¨ë¦¬ 50% ì ˆì•½)"
echo "5. --quantization: ëª¨ë¸ ì–‘ìí™” (AWQ, GPTQ ë“±)"
echo ""

# ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬
docker stop vllm-test1 vllm-test2 vllm-test3 2>/dev/null
docker rm vllm-test1 vllm-test2 vllm-test3 2>/dev/null

echo "ğŸ“Š ì´ˆê¸° GPU ìƒíƒœ:"
nvidia-smi --query-gpu=memory.used,memory.free --format=csv

# í…ŒìŠ¤íŠ¸ 1: ë§¤ìš° ë‚®ì€ ë©”ëª¨ë¦¬ ì‚¬ìš© (10%)
echo ""
echo "í…ŒìŠ¤íŠ¸ 1: ìµœì†Œ ë©”ëª¨ë¦¬ ì„¤ì • (gpu-util=0.05, max-len=512)"
docker run -d \
  --name vllm-test1 \
  --runtime nvidia \
  --gpus all \
  -p 41001:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  --shm-size 8g \
  vllm/vllm-openai:latest \
  --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --host 0.0.0.0 \
  --port 8000 \
  --gpu-memory-utilization 0.05 \
  --max-model-len 512 \
  --enforce-eager \
  --dtype float16

sleep 30
echo "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:"
nvidia-smi --query-gpu=memory.used --format=csv
docker ps | grep vllm-test1 || echo "ì‹¤íŒ¨"

# í…ŒìŠ¤íŠ¸ 2: KV ìºì‹œ íƒ€ì… ë³€ê²½ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
echo ""
echo "í…ŒìŠ¤íŠ¸ 2: FP8 KV ìºì‹œ ì‚¬ìš© (ë©”ëª¨ë¦¬ 50% ì ˆì•½)"
docker run -d \
  --name vllm-test2 \
  --runtime nvidia \
  --gpus all \
  -p 41002:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  --shm-size 8g \
  vllm/vllm-openai:latest \
  --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --host 0.0.0.0 \
  --port 8000 \
  --gpu-memory-utilization 0.1 \
  --max-model-len 1024 \
  --kv-cache-dtype fp8 \
  --enforce-eager

sleep 30
echo "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:"
nvidia-smi --query-gpu=memory.used --format=csv
docker ps | grep vllm-test2 || echo "ì‹¤íŒ¨"

# í…ŒìŠ¤íŠ¸ 3: í™˜ê²½ë³€ìˆ˜ë¡œ ì¶”ê°€ ì œì–´
echo ""
echo "í…ŒìŠ¤íŠ¸ 3: í™˜ê²½ë³€ìˆ˜ CUDA_VISIBLE_DEVICESë¡œ ë©”ëª¨ë¦¬ ì œí•œ ì‹œë®¬ë ˆì´ì…˜"
docker run -d \
  --name vllm-test3 \
  --runtime nvidia \
  --gpus all \
  -p 41003:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  -e PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:512,expandable_segments:True" \
  -e VLLM_ATTENTION_BACKEND=FLASHINFER \
  --shm-size 8g \
  vllm/vllm-openai:latest \
  --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --host 0.0.0.0 \
  --port 8000 \
  --gpu-memory-utilization 0.08 \
  --max-model-len 768 \
  --enforce-eager

sleep 30
echo "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:"
nvidia-smi --query-gpu=memory.used --format=csv
docker ps | grep vllm-test3 || echo "ì‹¤íŒ¨"

echo ""
echo "ğŸ“Š ìµœì¢… ìƒíƒœ:"
docker ps | grep vllm-test
nvidia-smi --query-gpu=memory.used,memory.free --format=csv

echo ""
echo "ğŸ’¡ vLLM ë©”ëª¨ë¦¬ ì œì–´ ì „ëµ:"
echo "1. gpu-memory-utilizationì„ 0.05ê¹Œì§€ ë‚®ì¶œ ìˆ˜ ìˆìŒ (SGLangì˜ mem-fraction-staticê³¼ ìœ ì‚¬)"
echo "2. max-model-lenì„ ì¤„ì—¬ KV ìºì‹œ í¬ê¸° ì œí•œ"
echo "3. kv-cache-dtypeì„ fp8ë¡œ ë³€ê²½í•˜ì—¬ ë©”ëª¨ë¦¬ 50% ì ˆì•½"
echo "4. enforce-eager ëª¨ë“œë¡œ ì¶”ê°€ ë©”ëª¨ë¦¬ ì ˆì•½"
echo "5. PYTORCH_CUDA_ALLOC_CONFë¡œ ë©”ëª¨ë¦¬ í• ë‹¹ ì„¸ë¶„í™”"
echo ""
echo "âš ï¸ ì°¨ì´ì :"
echo "- vLLM: ê°€ìš© ë©”ëª¨ë¦¬ì˜ ë¹„ìœ¨ë¡œ ê³„ì‚° (ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ ê³ ë ¤)"
echo "- SGLang: ì „ì²´ GPU ë©”ëª¨ë¦¬ì˜ ë¹„ìœ¨ë¡œ ê³„ì‚° (ë…ì ì )"