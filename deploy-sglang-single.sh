#!/bin/bash

echo "ğŸš€ SGLang ë‹¨ì¼ ëª¨ë¸ ë°°í¬ (ë©€í‹°ëª¨ë¸ ë¶ˆê°€ëŠ¥)"
echo ""

# ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬
echo "ğŸ§¹ ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬..."
docker stop sglang-tinyllama 2>/dev/null
docker rm sglang-tinyllama 2>/dev/null

# GPU ìƒíƒœ í™•ì¸
echo "ğŸ“Š ì´ˆê¸° GPU ìƒíƒœ:"
nvidia-smi --query-gpu=memory.used,memory.free,memory.total --format=csv

# TinyLlamaë§Œ ë°°í¬ (SGLangì€ í•œ ë²ˆì— í•˜ë‚˜ë§Œ ê°€ëŠ¥)
echo ""
echo "1ï¸âƒ£ TinyLlama 1.1B ë°°í¬ (SGLangì€ ë‹¨ì¼ ëª¨ë¸ë§Œ ì§€ì›)..."
docker run -d \
  --name sglang-tinyllama \
  --runtime nvidia \
  --gpus all \
  -p 30001:8000 \
  -v /root/.cache/huggingface:/root/.cache/huggingface \
  -e PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True" \
  --shm-size 16g \
  sglang:blackwell-final-v2 \
  --model-path TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --host 0.0.0.0 \
  --port 8000 \
  --mem-fraction-static 0.85 \
  --max-total-tokens 2048 \
  --max-running-requests 256 \
  --dtype float16 \
  --trust-remote-code \
  --disable-cuda-graph \
  --disable-custom-all-reduce \
  --disable-flashinfer \
  --disable-radix-cache \
  --attention-backend torch_native \
  --log-level info

echo "â³ SGLang ì´ˆê¸°í™” ì¤‘ (40ì´ˆ)..."
sleep 40

# ìƒíƒœ í™•ì¸
echo ""
echo "âœ… SGLang ë°°í¬ ì™„ë£Œ!"
echo ""
echo "ğŸ“Š ìµœì¢… GPU ë©”ëª¨ë¦¬ ìƒíƒœ:"
nvidia-smi --query-gpu=memory.used,memory.free,memory.total --format=csv

echo ""
echo "ğŸ³ ì‹¤í–‰ ì¤‘ì¸ ì»¨í…Œì´ë„ˆ:"
docker ps | grep sglang

echo ""
echo "ğŸ“¡ API ì—”ë“œí¬ì¸íŠ¸:"
echo "  - SGLang TinyLlama: http://localhost:30001/generate"

echo ""
echo "âš ï¸  SGLang ì œí•œì‚¬í•­ (RTX 5090):"
echo "  - ë‹¨ì¼ ëª¨ë¸ë§Œ ì‹¤í–‰ ê°€ëŠ¥ (ë©€í‹°ëª¨ë¸ ë¶ˆê°€)"
echo "  - RadixAttention ë¹„í™œì„±í™”ë¨ (RTX 5090 ë¯¸ì§€ì›)"
echo "  - ì—¬ëŸ¬ ìµœì í™” ê¸°ëŠ¥ ë¹„í™œì„±í™”ë¨"