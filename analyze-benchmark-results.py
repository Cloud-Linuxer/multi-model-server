#!/usr/bin/env python3
"""
Analyze and visualize benchmark results from CSV files
"""

import pandas as pd
import numpy as np
import json
import glob
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

def load_latest_results():
    """ìµœì‹  ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ íŒŒì¼ ì°¾ê¸°"""
    sglang_files = sorted(glob.glob("/home/multi-model-server/benchmark_sglang_*.csv"))
    vllm_files = sorted(glob.glob("/home/multi-model-server/benchmark_vllm_*.csv"))

    if not sglang_files or not vllm_files:
        print("âŒ No benchmark results found. Please run the benchmark first.")
        return None, None

    return sglang_files[-1], vllm_files[-1]

def analyze_detailed_metrics(sglang_df, vllm_df):
    """ìƒì„¸ ë©”íŠ¸ë¦­ ë¶„ì„"""
    results = {
        "summary": {},
        "language_comparison": {},
        "percentiles": {},
        "stability_analysis": {}
    }

    for framework, df in [("SGLang", sglang_df), ("vLLM", vllm_df)]:
        # ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ë§Œ ë¶„ì„
        success_df = df[df['success'] == True]

        # ê¸°ë³¸ í†µê³„
        results["summary"][framework] = {
            "total_tests": len(df),
            "successful_tests": len(success_df),
            "success_rate": f"{len(success_df) / len(df) * 100:.2f}%",
            "avg_ttft_ms": f"{success_df['ttft_ms'].mean():.2f}",
            "avg_latency_ms": f"{success_df['total_latency_ms'].mean():.2f}",
            "avg_throughput": f"{success_df['tokens_per_second'].mean():.2f}",
            "std_ttft": f"{success_df['ttft_ms'].std():.2f}",
            "std_latency": f"{success_df['total_latency_ms'].std():.2f}"
        }

        # ì–¸ì–´ë³„ ë¹„êµ
        for language in ['english', 'chinese', 'korean']:
            lang_df = success_df[success_df['language'] == language]
            if len(lang_df) > 0:
                if language not in results["language_comparison"]:
                    results["language_comparison"][language] = {}

                results["language_comparison"][language][framework] = {
                    "count": len(lang_df),
                    "avg_ttft_ms": f"{lang_df['ttft_ms'].mean():.2f}",
                    "avg_latency_ms": f"{lang_df['total_latency_ms'].mean():.2f}",
                    "avg_throughput": f"{lang_df['tokens_per_second'].mean():.2f}",
                    "p50_ttft": f"{lang_df['ttft_ms'].median():.2f}",
                    "p95_ttft": f"{lang_df['ttft_ms'].quantile(0.95):.2f}",
                    "p99_ttft": f"{lang_df['ttft_ms'].quantile(0.99):.2f}"
                }

        # ë°±ë¶„ìœ„ìˆ˜ ë¶„ì„
        results["percentiles"][framework] = {
            "ttft": {
                "p50": f"{success_df['ttft_ms'].quantile(0.50):.2f}",
                "p75": f"{success_df['ttft_ms'].quantile(0.75):.2f}",
                "p90": f"{success_df['ttft_ms'].quantile(0.90):.2f}",
                "p95": f"{success_df['ttft_ms'].quantile(0.95):.2f}",
                "p99": f"{success_df['ttft_ms'].quantile(0.99):.2f}"
            },
            "latency": {
                "p50": f"{success_df['total_latency_ms'].quantile(0.50):.2f}",
                "p75": f"{success_df['total_latency_ms'].quantile(0.75):.2f}",
                "p90": f"{success_df['total_latency_ms'].quantile(0.90):.2f}",
                "p95": f"{success_df['total_latency_ms'].quantile(0.95):.2f}",
                "p99": f"{success_df['total_latency_ms'].quantile(0.99):.2f}"
            }
        }

        # ì•ˆì •ì„± ë¶„ì„ (ë³€ë™ ê³„ìˆ˜ = í‘œì¤€í¸ì°¨/í‰ê· )
        cv_ttft = success_df['ttft_ms'].std() / success_df['ttft_ms'].mean() * 100
        cv_latency = success_df['total_latency_ms'].std() / success_df['total_latency_ms'].mean() * 100

        results["stability_analysis"][framework] = {
            "cv_ttft": f"{cv_ttft:.2f}%",
            "cv_latency": f"{cv_latency:.2f}%",
            "min_ttft": f"{success_df['ttft_ms'].min():.2f}",
            "max_ttft": f"{success_df['ttft_ms'].max():.2f}",
            "range_ttft": f"{success_df['ttft_ms'].max() - success_df['ttft_ms'].min():.2f}"
        }

    return results

def create_markdown_report(analysis_results):
    """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
    report = []
    report.append("# ğŸ”¬ Multilingual Performance Benchmark Analysis Report")
    report.append(f"\n**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("\n## ğŸ“Š Executive Summary\n")

    # ì „ì²´ ìš”ì•½
    report.append("### Overall Performance Comparison\n")
    report.append("| Metric | SGLang | vLLM | Winner |")
    report.append("|--------|--------|------|--------|")

    sglang_summary = analysis_results["summary"]["SGLang"]
    vllm_summary = analysis_results["summary"]["vLLM"]

    # Success Rate
    sg_rate = float(sglang_summary["success_rate"].rstrip('%'))
    vl_rate = float(vllm_summary["success_rate"].rstrip('%'))
    winner = "SGLang" if sg_rate > vl_rate else "vLLM" if vl_rate > sg_rate else "Tie"
    report.append(f"| Success Rate | {sglang_summary['success_rate']} | {vllm_summary['success_rate']} | {winner} |")

    # TTFT
    sg_ttft = float(sglang_summary["avg_ttft_ms"])
    vl_ttft = float(vllm_summary["avg_ttft_ms"])
    winner = "SGLang âš¡" if sg_ttft < vl_ttft else "vLLM âš¡" if vl_ttft < sg_ttft else "Tie"
    report.append(f"| Avg TTFT (ms) | {sglang_summary['avg_ttft_ms']} | {vllm_summary['avg_ttft_ms']} | {winner} |")

    # Latency
    sg_lat = float(sglang_summary["avg_latency_ms"])
    vl_lat = float(vllm_summary["avg_latency_ms"])
    winner = "SGLang âš¡" if sg_lat < vl_lat else "vLLM âš¡" if vl_lat < sg_lat else "Tie"
    report.append(f"| Avg Latency (ms) | {sglang_summary['avg_latency_ms']} | {vllm_summary['avg_latency_ms']} | {winner} |")

    # Throughput
    sg_thr = float(sglang_summary["avg_throughput"])
    vl_thr = float(vllm_summary["avg_throughput"])
    winner = "SGLang ğŸš€" if sg_thr > vl_thr else "vLLM ğŸš€" if vl_thr > sg_thr else "Tie"
    report.append(f"| Avg Throughput (tok/s) | {sglang_summary['avg_throughput']} | {vllm_summary['avg_throughput']} | {winner} |")

    # ì–¸ì–´ë³„ ì„±ëŠ¥
    report.append("\n## ğŸŒ Language-Specific Performance\n")

    for language in ['english', 'chinese', 'korean']:
        lang_title = language.capitalize()
        report.append(f"### {lang_title}\n")
        report.append("| Metric | SGLang | vLLM | Winner |")
        report.append("|--------|--------|------|--------|")

        if language in analysis_results["language_comparison"]:
            sg_lang = analysis_results["language_comparison"][language].get("SGLang", {})
            vl_lang = analysis_results["language_comparison"][language].get("vLLM", {})

            if sg_lang and vl_lang:
                # TTFT
                sg_val = float(sg_lang["avg_ttft_ms"])
                vl_val = float(vl_lang["avg_ttft_ms"])
                winner = "SGLang" if sg_val < vl_val else "vLLM" if vl_val < sg_val else "Tie"
                report.append(f"| Avg TTFT (ms) | {sg_lang['avg_ttft_ms']} | {vl_lang['avg_ttft_ms']} | {winner} |")

                # Latency
                sg_val = float(sg_lang["avg_latency_ms"])
                vl_val = float(vl_lang["avg_latency_ms"])
                winner = "SGLang" if sg_val < vl_val else "vLLM" if vl_val < sg_val else "Tie"
                report.append(f"| Avg Latency (ms) | {sg_lang['avg_latency_ms']} | {vl_lang['avg_latency_ms']} | {winner} |")

                # Throughput
                sg_val = float(sg_lang["avg_throughput"])
                vl_val = float(vl_lang["avg_throughput"])
                winner = "SGLang" if sg_val > vl_val else "vLLM" if vl_val > sg_val else "Tie"
                report.append(f"| Throughput (tok/s) | {sg_lang['avg_throughput']} | {vl_lang['avg_throughput']} | {winner} |")

                # P95 TTFT
                report.append(f"| P95 TTFT (ms) | {sg_lang['p95_ttft']} | {vl_lang['p95_ttft']} | - |")

        report.append("")

    # ë°±ë¶„ìœ„ìˆ˜ ë¶„ì„
    report.append("\n## ğŸ“ˆ Percentile Analysis\n")
    report.append("### Time to First Token (TTFT)\n")
    report.append("| Percentile | SGLang (ms) | vLLM (ms) |")
    report.append("|------------|-------------|-----------|")

    for p in ["p50", "p75", "p90", "p95", "p99"]:
        sg_val = analysis_results["percentiles"]["SGLang"]["ttft"][p]
        vl_val = analysis_results["percentiles"]["vLLM"]["ttft"][p]
        report.append(f"| {p.upper()} | {sg_val} | {vl_val} |")

    # ì•ˆì •ì„± ë¶„ì„
    report.append("\n## ğŸ¯ Stability Analysis\n")
    report.append("| Metric | SGLang | vLLM | More Stable |")
    report.append("|--------|--------|------|-------------|")

    sg_cv = float(analysis_results["stability_analysis"]["SGLang"]["cv_ttft"].rstrip('%'))
    vl_cv = float(analysis_results["stability_analysis"]["vLLM"]["cv_ttft"].rstrip('%'))
    winner = "SGLang âœ…" if sg_cv < vl_cv else "vLLM âœ…" if vl_cv < sg_cv else "Tie"
    report.append(f"| TTFT CV | {analysis_results['stability_analysis']['SGLang']['cv_ttft']} | {analysis_results['stability_analysis']['vLLM']['cv_ttft']} | {winner} |")

    sg_cv = float(analysis_results["stability_analysis"]["SGLang"]["cv_latency"].rstrip('%'))
    vl_cv = float(analysis_results["stability_analysis"]["vLLM"]["cv_latency"].rstrip('%'))
    winner = "SGLang âœ…" if sg_cv < vl_cv else "vLLM âœ…" if vl_cv < sg_cv else "Tie"
    report.append(f"| Latency CV | {analysis_results['stability_analysis']['SGLang']['cv_latency']} | {analysis_results['stability_analysis']['vLLM']['cv_latency']} | {winner} |")

    # ê²°ë¡ 
    report.append("\n## ğŸ† Final Verdict\n")

    # ì ìˆ˜ ê³„ì‚°
    sglang_wins = 0
    vllm_wins = 0

    # TTFT ë¹„êµ
    if float(sglang_summary["avg_ttft_ms"]) < float(vllm_summary["avg_ttft_ms"]):
        sglang_wins += 1
    else:
        vllm_wins += 1

    # Throughput ë¹„êµ
    if float(sglang_summary["avg_throughput"]) > float(vllm_summary["avg_throughput"]):
        sglang_wins += 1
    else:
        vllm_wins += 1

    # Stability ë¹„êµ
    if sg_cv < vl_cv:
        sglang_wins += 1
    else:
        vllm_wins += 1

    if vllm_wins > sglang_wins:
        report.append("### ğŸ¥‡ **Winner: vLLM**\n")
        report.append(f"vLLM demonstrated superior performance in {vllm_wins} out of 3 key metrics.")
    elif sglang_wins > vllm_wins:
        report.append("### ğŸ¥‡ **Winner: SGLang**\n")
        report.append(f"SGLang demonstrated superior performance in {sglang_wins} out of 3 key metrics.")
    else:
        report.append("### ğŸ¤ **Result: Tie**\n")
        report.append("Both frameworks showed comparable performance across key metrics.")

    report.append("\n### Key Findings:\n")

    # TTFT ë¹„êµ
    ttft_diff = abs(float(sglang_summary["avg_ttft_ms"]) - float(vllm_summary["avg_ttft_ms"]))
    ttft_winner = "SGLang" if float(sglang_summary["avg_ttft_ms"]) < float(vllm_summary["avg_ttft_ms"]) else "vLLM"
    report.append(f"- **Time to First Token**: {ttft_winner} is {ttft_diff:.1f}ms faster")

    # Throughput ë¹„êµ
    thr_diff = abs(float(sglang_summary["avg_throughput"]) - float(vllm_summary["avg_throughput"]))
    thr_winner = "SGLang" if float(sglang_summary["avg_throughput"]) > float(vllm_summary["avg_throughput"]) else "vLLM"
    report.append(f"- **Throughput**: {thr_winner} achieves {thr_diff:.1f} tok/s higher throughput")

    # Stability ë¹„êµ
    stability_winner = "SGLang" if sg_cv < vl_cv else "vLLM"
    report.append(f"- **Stability**: {stability_winner} shows more consistent performance")

    # ì–¸ì–´ë³„ ìµœì 
    report.append("\n### Language-Specific Recommendations:\n")
    for language in ['english', 'chinese', 'korean']:
        if language in analysis_results["language_comparison"]:
            sg_lang = analysis_results["language_comparison"][language].get("SGLang", {})
            vl_lang = analysis_results["language_comparison"][language].get("vLLM", {})
            if sg_lang and vl_lang:
                sg_thr = float(sg_lang["avg_throughput"])
                vl_thr = float(vl_lang["avg_throughput"])
                winner = "SGLang" if sg_thr > vl_thr else "vLLM"
                report.append(f"- **{language.capitalize()}**: {winner} performs better ({max(sg_thr, vl_thr):.1f} tok/s)")

    report.append("\n---\n")
    report.append(f"*Benchmark conducted on RTX 5090 (32GB) with TinyLlama 1.1B*")
    report.append(f"*Total tests per framework: 1800 (120 iterations Ã— 15 prompts)*")

    return "\n".join(report)

def main():
    print("ğŸ“Š Analyzing benchmark results...")

    # ìµœì‹  ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
    sglang_file, vllm_file = load_latest_results()

    if not sglang_file or not vllm_file:
        print("Waiting for benchmark results...")
        return

    print(f"Found SGLang results: {sglang_file}")
    print(f"Found vLLM results: {vllm_file}")

    # ë°ì´í„° ë¡œë“œ
    sglang_df = pd.read_csv(sglang_file)
    vllm_df = pd.read_csv(vllm_file)

    print(f"SGLang tests: {len(sglang_df)}")
    print(f"vLLM tests: {len(vllm_df)}")

    # ìƒì„¸ ë¶„ì„
    analysis = analyze_detailed_metrics(sglang_df, vllm_df)

    # JSON ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    analysis_file = f"/home/multi-model-server/detailed_analysis_{timestamp}.json"
    with open(analysis_file, "w") as f:
        json.dump(analysis, f, indent=2)
    print(f"ğŸ’¾ Detailed analysis saved to {analysis_file}")

    # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
    report = create_markdown_report(analysis)
    report_file = f"/home/multi-model-server/BENCHMARK_REPORT_{timestamp}.md"
    with open(report_file, "w") as f:
        f.write(report)
    print(f"ğŸ“ Markdown report saved to {report_file}")

    # ì½˜ì†”ì—ë„ ì¶œë ¥
    print("\n" + "="*80)
    print(report)
    print("="*80)

if __name__ == "__main__":
    main()