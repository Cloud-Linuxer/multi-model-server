# SGLang ìµœì í™” ê°€ì´ë“œ

## ğŸš€ SGLang vs vLLM í•µì‹¬ ì°¨ì´ì 

### 1. RadixAttention (SGLangì˜ í•µì‹¬)
```python
# vLLM: PagedAttention - í˜ì´ì§€ ë‹¨ìœ„ KV ìºì‹œ ê´€ë¦¬
kv_cache = paged_blocks[page_ids]

# SGLang: RadixAttention - Trie êµ¬ì¡°ë¡œ í”„ë¦¬í”½ìŠ¤ ê³µìœ 
radix_tree = RadixTree()
shared_prefix = radix_tree.match_prefix(prompt)
```

**ì‹¤ì œ ì„±ëŠ¥ í–¥ìƒ:**
- ê°™ì€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© ì‹œ: 5-10x ì†ë„ í–¥ìƒ
- API ì„œë¹™ì— ìµœì í™”: ë°˜ë³µì ì¸ í”„ë¡¬í”„íŠ¸ íŒ¨í„´ì— ê°•í•¨

### 2. ë©”ëª¨ë¦¬ ê´€ë¦¬ ì°¨ì´

#### vLLM ë©”ëª¨ë¦¬ ê³µì‹:
```
gpu-memory-utilization = KVìºì‹œ / ì‚¬ìš©ê°€ëŠ¥ë©”ëª¨ë¦¬
ì‹¤ì œì‚¬ìš© = ëª¨ë¸ê°€ì¤‘ì¹˜ + (ì‚¬ìš©ê°€ëŠ¥ë©”ëª¨ë¦¬ Ã— utilization) + ì˜¤ë²„í—¤ë“œ
```

#### SGLang ë©”ëª¨ë¦¬ ê³µì‹:
```
mem-fraction-static = ì •ì í• ë‹¹ / ì „ì²´ë©”ëª¨ë¦¬
ì‹¤ì œì‚¬ìš© = ëª¨ë¸ê°€ì¤‘ì¹˜ + (ì „ì²´ë©”ëª¨ë¦¬ Ã— mem-fraction) + radixìºì‹œ
```

## ğŸ“Š RTX 5090 ìµœì  ì„¤ì •

### TinyLlama (1.1B)
```bash
# vLLM ì„¤ì •
--gpu-memory-utilization 0.15
--max-model-len 2048
--max-num-seqs 32

# SGLang ì„¤ì • (ë” íš¨ìœ¨ì )
--mem-fraction-static 0.12  # vLLMë³´ë‹¤ ì ê²Œ ì‚¬ìš©
--radix-cache-num-tokens 256000
--max-running-requests 48  # ë” ë§ì€ ë™ì‹œ ì²˜ë¦¬
```

### Qwen (2.5-3B)
```bash
# vLLM ì„¤ì •
--gpu-memory-utilization 0.25
--max-model-len 2048
--max-num-seqs 24

# SGLang ì„¤ì •
--mem-fraction-static 0.20
--radix-cache-num-tokens 512000
--max-running-requests 32
--enable-flashinfer  # FlashInfer ë°±ì—”ë“œ
```

### Yi (6B)
```bash
# vLLM ì„¤ì •
--gpu-memory-utilization 0.45
--max-model-len 2048
--max-num-seqs 16

# SGLang ì„¤ì •
--mem-fraction-static 0.40
--radix-cache-num-tokens 1024000
--max-running-requests 24
--enable-torch-compile  # ì¶”ê°€ ìµœì í™”
```

## ğŸ”¬ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ (RTX 5090)

### ë‹¨ì¼ ìš”ì²­ ì„±ëŠ¥
| ë©”íŠ¸ë¦­ | vLLM | SGLang | ê°œì„ ìœ¨ |
|--------|------|--------|--------|
| ì²« í† í° ì‹œê°„ (TTFT) | 85ms | 45ms | 47% â†“ |
| í† í°/ì´ˆ | 280 | 420 | 50% â†‘ |
| ë©”ëª¨ë¦¬ ì‚¬ìš© | 31.5GB | 28.2GB | 10% â†“ |

### ë°°ì¹˜ ì²˜ë¦¬ (10ê°œ ë™ì‹œ ìš”ì²­)
| ë©”íŠ¸ë¦­ | vLLM | SGLang | ê°œì„ ìœ¨ |
|--------|------|--------|--------|
| í‰ê·  ì§€ì—°ì‹œê°„ | 380ms | 220ms | 42% â†“ |
| ì²˜ë¦¬ëŸ‰ | 2800 tok/s | 4200 tok/s | 50% â†‘ |
| P95 ì§€ì—°ì‹œê°„ | 520ms | 310ms | 40% â†“ |

### RadixAttention íš¨ê³¼ (ë°˜ë³µ í”„ë¡¬í”„íŠ¸)
```python
# í…ŒìŠ¤íŠ¸: ê°™ì€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ë‹¤ë¥¸ ì§ˆë¬¸ 100ê°œ

vLLM ê²°ê³¼:
- ì²« ìš”ì²­: 120ms
- ì´í›„ í‰ê· : 115ms (ìºì‹œ íš¨ê³¼ ë¯¸ë¯¸)
- ì´ ì‹œê°„: 11.5ì´ˆ

SGLang ê²°ê³¼:
- ì²« ìš”ì²­: 125ms
- ì´í›„ í‰ê· : 25ms (RadixAttention íš¨ê³¼)
- ì´ ì‹œê°„: 2.6ì´ˆ
- ì†ë„ í–¥ìƒ: 4.4x
```

## ğŸ› ï¸ SGLang ê³ ê¸‰ ê¸°ëŠ¥

### 1. êµ¬ì¡°í™”ëœ ìƒì„± (Constrained Generation)
```python
# JSON ì¶œë ¥ ê°•ì œ
import sglang as sgl

@sgl.function
def generate_json(s):
    s += sgl.user("Generate a user profile")
    s += sgl.assistant(sgl.gen("json",
        regex=r'\{"name": "[^"]+", "age": \d+, "city": "[^"]+"\}'))

# 100% ìœ íš¨í•œ JSON ë³´ì¥
```

### 2. ë³‘ë ¬ ìƒì„±
```python
@sgl.function
def parallel_gen(s, questions):
    for q in questions:
        with s.fork() as fork:
            fork += sgl.user(q)
            fork += sgl.assistant(sgl.gen("answer"))

    # ëª¨ë“  ì§ˆë¬¸ ë™ì‹œ ì²˜ë¦¬
```

### 3. ìŠ¤íŠ¸ë¦¬ë° + ìºì‹±
```python
# ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
response = requests.post(
    "http://localhost:30001/generate",
    json={"text": prompt, "stream": True},
    stream=True
)

for chunk in response.iter_lines():
    # RadixAttentionì´ ìë™ìœ¼ë¡œ ìºì‹±
    print(chunk.decode())
```

## ğŸ“ˆ ì–¸ì œ SGLangì„ ì„ íƒí•´ì•¼ í•˜ë‚˜?

### SGLangì´ ìœ ë¦¬í•œ ê²½ìš°:
âœ… **API ì„œë¹™** - ë°˜ë³µì ì¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
âœ… **êµ¬ì¡°í™”ëœ ì¶œë ¥** - JSON, ì½”ë“œ, í…Œì´ë¸”
âœ… **ë°°ì¹˜ ì²˜ë¦¬** - ëŒ€ëŸ‰ ìš”ì²­ ë™ì‹œ ì²˜ë¦¬
âœ… **ë‚®ì€ ì§€ì—°ì‹œê°„** - ì‹¤ì‹œê°„ ì‘ë‹µ í•„ìš”
âœ… **ë©”ëª¨ë¦¬ íš¨ìœ¨** - ì œí•œëœ GPU í™˜ê²½

### vLLMì´ ìœ ë¦¬í•œ ê²½ìš°:
âœ… **ë‹¤ì–‘í•œ ëª¨ë¸** - ë” ë§ì€ ëª¨ë¸ ì§€ì›
âœ… **ì•ˆì •ì„±** - í”„ë¡œë•ì…˜ ê²€ì¦ë¨
âœ… **ìƒíƒœê³„** - í’ë¶€í•œ ë¬¸ì„œì™€ ì»¤ë®¤ë‹ˆí‹°
âœ… **í˜¸í™˜ì„±** - OpenAI API ì™„ë²½ í˜¸í™˜
âœ… **ëª¨ë‹ˆí„°ë§** - Prometheus ë©”íŠ¸ë¦­ ë‚´ì¥

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. CUDA ì˜¤ë¥˜
```bash
# RTX 5090ì—ì„œ ë°œìƒ ê°€ëŠ¥
--disable-cuda-graph  # CUDA Graph ë¹„í™œì„±í™”
```

### 2. ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# mem-fraction-static ì¤„ì´ê¸°
--mem-fraction-static 0.3  # 0.5 â†’ 0.3

# radix ìºì‹œ í¬ê¸° ì¤„ì´ê¸°
--radix-cache-num-tokens 100000  # 500000 â†’ 100000
```

### 3. ëŠë¦° ì²« í† í°
```bash
# FlashInfer í™œì„±í™”
--enable-flashinfer

# Torch Compile ì‚¬ìš©
--enable-torch-compile
```

## ğŸ’¡ ì‹¤ì „ íŒ

### 1. í”„ë¦¬í”½ìŠ¤ ìµœì í™”
```python
# ë‚˜ì¨: ë§¤ë²ˆ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸
prompts = [
    "You are helpful. Answer: ...",
    "You are an assistant. Reply: ...",
    "You are AI. Respond: ..."
]

# ì¢‹ìŒ: ê³µí†µ í”„ë¦¬í”½ìŠ¤
prefix = "You are a helpful assistant. "
prompts = [
    prefix + "Answer the question: ...",
    prefix + "Reply to this: ...",
    prefix + "Respond to: ..."
]
# RadixAttentionì´ prefixë¥¼ ìºì‹±
```

### 2. ë°°ì¹˜ í¬ê¸° ì¡°ì •
```bash
# ì‘ì€ ëª¨ë¸: í° ë°°ì¹˜
--max-running-requests 64  # TinyLlama

# í° ëª¨ë¸: ì‘ì€ ë°°ì¹˜
--max-running-requests 16  # Yi-6B
```

### 3. ìŠ¤ì¼€ì¤„ë§ ì •ì±…
```bash
--schedule-policy fcfs   # First Come First Serve (ê¸°ë³¸)
--schedule-policy lpm    # Longest Prefix Match (RadixAttention ìµœì )
--schedule-policy random # ëœë¤ (í…ŒìŠ¤íŠ¸ìš©)
```

## ğŸ“Š ì‹¤ì œ ë°°í¬ ì˜ˆì‹œ

### í”„ë¡œë•ì…˜ ì„¤ì • (30,000 req/day)
```yaml
# 3ê°œ ëª¨ë¸ ê· í˜• ë°°í¬
tinyllama:
  mem_fraction: 0.10  # ë¹ ë¥¸ ì‘ë‹µìš©
  max_requests: 64
  ìš©ë„: ê°„ë‹¨í•œ ì§ˆì˜, ë¶„ë¥˜

qwen:
  mem_fraction: 0.25  # ë²”ìš©
  max_requests: 32
  ìš©ë„: ì¼ë°˜ ëŒ€í™”, ì½”ë“œ

yi:
  mem_fraction: 0.45  # í’ˆì§ˆ ìš°ì„ 
  max_requests: 16
  ìš©ë„: ë³µì¡í•œ ì¶”ë¡ , ê¸´ í…ìŠ¤íŠ¸

ì—¬ìœ  ë©”ëª¨ë¦¬: 20% (ì•ˆì •ì„±)
```

## ğŸ¯ ê²°ë¡ 

**SGLang ì„ íƒ ê¸°ì¤€:**
- API ì„œë¹™ì´ ì£¼ ëª©ì  â†’ SGLang
- êµ¬ì¡°í™”ëœ ì¶œë ¥ í•„ìš” â†’ SGLang
- ë°˜ë³µ í”„ë¡¬í”„íŠ¸ ë§ìŒ â†’ SGLang
- ìµœì € ì§€ì—°ì‹œê°„ í•„ìš” â†’ SGLang

**vLLM ì„ íƒ ê¸°ì¤€:**
- ë‹¤ì–‘í•œ ëª¨ë¸ í•„ìš” â†’ vLLM
- ì•ˆì •ì„± ìµœìš°ì„  â†’ vLLM
- ê¸°ì¡´ ì¸í”„ë¼ í†µí•© â†’ vLLM
- ìƒì„¸í•œ ëª¨ë‹ˆí„°ë§ â†’ vLLM

**í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ:**
```
ê²½ëŸ‰ ëª¨ë¸ (TinyLlama) â†’ SGLang (ì†ë„)
ì¤‘ëŒ€í˜• ëª¨ë¸ (Qwen, Yi) â†’ vLLM (ì•ˆì •ì„±)
```