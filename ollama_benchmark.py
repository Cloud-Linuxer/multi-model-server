#!/usr/bin/env python3
import requests
import time
import csv
from datetime import datetime
import json

# í…ŒìŠ¤íŠ¸ ì„¤ì •
NUM_TESTS = 100
BASE_URL = "http://localhost:11434/api/generate"

# í”„ë¡¬í”„íŠ¸ ì„¤ì •
prompts = {
    "english": "Sing a beautiful song about spring with blooming flowers.",
    "chinese": "å”±ä¸€é¦–å…³äºæ˜¥å¤©ç™¾èŠ±ç››å¼€ä¹‹ç¾çš„æ­Œæ›²ã€‚",
    "korean": "ë´„ì— í”¼ì–´ë‚˜ëŠ” ê½ƒë“¤ì˜ ì•„ë¦„ë‹¤ì›€ì„ ë…¸ë˜í•´ì£¼ì„¸ìš”."
}

def test_ollama(model, prompt, language, iteration):
    """Ollama API í…ŒìŠ¤íŠ¸"""
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "num_predict": 100,
            "temperature": 0.7
        }
    }

    start_time = time.time()
    try:
        response = requests.post(BASE_URL, json=payload, timeout=30)
        end_time = time.time()

        if response.status_code == 200:
            result = response.json()
            response_text = result.get('response', '')
            total_tokens = len(response_text.split())  # ê°„ë‹¨í•œ í† í° ê³„ì‚°

            latency = (end_time - start_time) * 1000  # ms
            throughput = total_tokens / (end_time - start_time) if end_time > start_time else 0

            return {
                "timestamp": datetime.now().isoformat(),
                "iteration": iteration,
                "language": language,
                "model": model,
                "prompt": prompt[:50] + "...",
                "response_preview": response_text[:100] + "...",
                "latency_ms": round(latency, 2),
                "total_tokens": total_tokens,
                "throughput_tok_s": round(throughput, 2),
                "status": "success",
                "error": None
            }
    except Exception as e:
        return {
            "timestamp": datetime.now().isoformat(),
            "iteration": iteration,
            "language": language,
            "model": model,
            "prompt": prompt[:50] + "...",
            "response_preview": None,
            "latency_ms": None,
            "total_tokens": 0,
            "throughput_tok_s": 0,
            "status": "failed",
            "error": str(e)
        }

def main():
    # CSV íŒŒì¼ëª… ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"ollama_benchmark_{timestamp}.csv"

    # ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
    results = []

    print(f"ğŸš€ Ollama ë©€í‹°ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ìˆ˜: {NUM_TESTS}ê°œ (ì–¸ì–´ë³„)")
    print(f"ğŸ’¾ ê²°ê³¼ íŒŒì¼: {csv_filename}\n")

    # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ë“¤
    models = ["tinyllama:1.1b", "qwen2.5:3b", "yi:6b"]

    for model in models:
        print(f"\nğŸ”¬ {model} í…ŒìŠ¤íŠ¸ ì¤‘...")
        model_results = []

        for lang, prompt in prompts.items():
            print(f"  ğŸ“ {lang} í…ŒìŠ¤íŠ¸ ì¤‘...")
            for i in range(1, NUM_TESTS + 1):
                result = test_ollama(model, prompt, lang, i)
                results.append(result)
                model_results.append(result)

                if i % 10 == 0:
                    success_count = sum(1 for r in model_results if r['status'] == 'success')
                    avg_latency = sum(r['latency_ms'] for r in model_results if r['latency_ms']) / len([r for r in model_results if r['latency_ms']])
                    print(f"    ì§„í–‰: {i}/{NUM_TESTS} - ì„±ê³µë¥ : {success_count}/{len(model_results)} - í‰ê·  ì§€ì—°: {avg_latency:.2f}ms")

    # CSV íŒŒì¼ ì €ì¥
    if results:
        fieldnames = results[0].keys()
        with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(results)

        print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ê²°ê³¼ê°€ {csv_filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # í†µê³„ ì¶œë ¥
        print("\nğŸ“Š ì „ì²´ í†µê³„:")
        for model in models:
            model_results = [r for r in results if r['model'] == model]
            if model_results:
                success_results = [r for r in model_results if r['status'] == 'success']
                if success_results:
                    avg_latency = sum(r['latency_ms'] for r in success_results) / len(success_results)
                    avg_throughput = sum(r['throughput_tok_s'] for r in success_results) / len(success_results)
                    success_rate = (len(success_results) / len(model_results)) * 100

                    print(f"\n{model}:")
                    print(f"  ì„±ê³µë¥ : {success_rate:.1f}%")
                    print(f"  í‰ê·  ì§€ì—°ì‹œê°„: {avg_latency:.2f}ms")
                    print(f"  í‰ê·  ì²˜ë¦¬ëŸ‰: {avg_throughput:.2f} tok/s")

if __name__ == "__main__":
    main()