#!/usr/bin/env python3
"""
SGLang vs vLLM ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸
RTX 5090ì—ì„œ ì‹¤í–‰
"""

import time
import json
import requests
from typing import Dict, List
import statistics

# ì„œë²„ ì„¤ì •
SERVERS = {
    "vLLM": {
        "TinyLlama": "http://localhost:8001/v1/completions",
        "Qwen-3B": "http://localhost:8002/v1/completions",
        "Yi-6B": "http://localhost:8003/v1/completions"
    },
    "SGLang": {
        "TinyLlama": "http://localhost:30001/generate",
        # SGLangì€ ë‹¨ì¼ ëª¨ë¸ë§Œ ì‹¤í–‰ ê°€ëŠ¥ (ë©”ëª¨ë¦¬ ì œí•œ)
    }
}

# í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
PROMPTS = [
    "What is artificial intelligence?",
    "Explain quantum computing in simple terms",
    "Write a Python function to sort a list",
    "What are the benefits of renewable energy?",
    "Describe the water cycle",
]

def test_vllm(url: str, prompt: str, max_tokens: int = 100) -> Dict:
    """vLLM API í…ŒìŠ¤íŠ¸"""
    start = time.time()

    payload = {
        "model": "model",
        "prompt": prompt,
        "max_tokens": max_tokens,
        "temperature": 0.7
    }

    try:
        response = requests.post(url, json=payload, timeout=30)
        response.raise_for_status()

        end = time.time()
        result = response.json()

        return {
            "success": True,
            "latency": (end - start) * 1000,  # ms
            "tokens": len(result["choices"][0]["text"].split()),
            "text": result["choices"][0]["text"][:100] + "..."
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "latency": 0,
            "tokens": 0
        }

def test_sglang(url: str, prompt: str, max_tokens: int = 100) -> Dict:
    """SGLang API í…ŒìŠ¤íŠ¸"""
    start = time.time()

    payload = {
        "text": prompt,
        "max_new_tokens": max_tokens,
        "temperature": 0.7
    }

    try:
        response = requests.post(url, json=payload, timeout=30)
        response.raise_for_status()

        end = time.time()
        result = response.json()

        return {
            "success": True,
            "latency": (end - start) * 1000,  # ms
            "e2e_latency": result.get("meta_info", {}).get("e2e_latency", 0) * 1000,
            "tokens": result.get("meta_info", {}).get("completion_tokens", 0),
            "text": result.get("text", "")[:100] + "..."
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "latency": 0,
            "tokens": 0
        }

def run_benchmark():
    """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    print("=" * 80)
    print("ğŸš€ SGLang vs vLLM ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("=" * 80)

    results = {}

    # vLLM í…ŒìŠ¤íŠ¸
    print("\nğŸ“Š vLLM í…ŒìŠ¤íŠ¸ ì¤‘...")
    for model_name, url in SERVERS["vLLM"].items():
        print(f"\n  ğŸ”¹ {model_name} í…ŒìŠ¤íŠ¸...")
        model_results = []

        for prompt in PROMPTS:
            result = test_vllm(url, prompt)
            model_results.append(result)
            if result["success"]:
                print(f"    âœ… Latency: {result['latency']:.2f}ms, Tokens: {result['tokens']}")
            else:
                print(f"    âŒ Error: {result['error']}")

        # í†µê³„ ê³„ì‚°
        successful_results = [r for r in model_results if r["success"]]
        if successful_results:
            avg_latency = statistics.mean([r["latency"] for r in successful_results])
            avg_tokens = statistics.mean([r["tokens"] for r in successful_results])

            results[f"vLLM_{model_name}"] = {
                "avg_latency_ms": avg_latency,
                "avg_tokens": avg_tokens,
                "success_rate": len(successful_results) / len(model_results) * 100,
                "total_tests": len(model_results)
            }

            print(f"    ğŸ“ˆ í‰ê· : {avg_latency:.2f}ms, {avg_tokens:.1f} tokens")

    # SGLang í…ŒìŠ¤íŠ¸ (ë‹¨ì¼ ëª¨ë¸ë§Œ)
    print("\nğŸ“Š SGLang í…ŒìŠ¤íŠ¸ ì¤‘...")
    for model_name, url in SERVERS["SGLang"].items():
        print(f"\n  ğŸ”¹ {model_name} í…ŒìŠ¤íŠ¸...")
        model_results = []

        for prompt in PROMPTS:
            result = test_sglang(url, prompt)
            model_results.append(result)
            if result["success"]:
                print(f"    âœ… Latency: {result['latency']:.2f}ms, E2E: {result['e2e_latency']:.2f}ms, Tokens: {result['tokens']}")
            else:
                print(f"    âŒ Error: {result['error']}")

        # í†µê³„ ê³„ì‚°
        successful_results = [r for r in model_results if r["success"]]
        if successful_results:
            avg_latency = statistics.mean([r["latency"] for r in successful_results])
            avg_e2e = statistics.mean([r["e2e_latency"] for r in successful_results if r.get("e2e_latency")])
            avg_tokens = statistics.mean([r["tokens"] for r in successful_results])

            results[f"SGLang_{model_name}"] = {
                "avg_latency_ms": avg_latency,
                "avg_e2e_latency_ms": avg_e2e,
                "avg_tokens": avg_tokens,
                "success_rate": len(successful_results) / len(model_results) * 100,
                "total_tests": len(model_results)
            }

            print(f"    ğŸ“ˆ í‰ê· : {avg_latency:.2f}ms, E2E: {avg_e2e:.2f}ms, {avg_tokens:.1f} tokens")

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 80)
    print("ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½")
    print("=" * 80)

    print("\nğŸ“‹ ì„±ëŠ¥ ë¹„êµí‘œ:")
    print(f"{'Framework':<20} {'Model':<15} {'Avg Latency (ms)':<20} {'Success Rate':<15}")
    print("-" * 70)

    for key, value in results.items():
        framework, model = key.split("_", 1)
        print(f"{framework:<20} {model:<15} {value['avg_latency_ms']:<20.2f} {value['success_rate']:<15.1f}%")

    # JSON íŒŒì¼ë¡œ ì €ì¥
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    filename = f"benchmark_results_{timestamp}.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {filename}")

    # ë¹„êµ ë¶„ì„
    print("\nğŸ” ë¶„ì„:")

    # TinyLlama ë¹„êµ (ë‘˜ ë‹¤ ì‹¤í–‰ ê°€ëŠ¥í•œ ê²½ìš°)
    if "vLLM_TinyLlama" in results and "SGLang_TinyLlama" in results:
        vllm_latency = results["vLLM_TinyLlama"]["avg_latency_ms"]
        sglang_latency = results["SGLang_TinyLlama"]["avg_latency_ms"]

        if sglang_latency < vllm_latency:
            improvement = ((vllm_latency - sglang_latency) / vllm_latency) * 100
            print(f"  âœ… SGLangì´ vLLMë³´ë‹¤ {improvement:.1f}% ë¹ ë¦„ (TinyLlama)")
        else:
            degradation = ((sglang_latency - vllm_latency) / vllm_latency) * 100
            print(f"  âš ï¸ SGLangì´ vLLMë³´ë‹¤ {degradation:.1f}% ëŠë¦¼ (TinyLlama)")

    print("\nğŸ“ ì£¼ìš” ë°œê²¬ì‚¬í•­:")
    print("  - vLLM: 3ê°œ ëª¨ë¸ ë™ì‹œ ì‹¤í–‰ ê°€ëŠ¥")
    print("  - SGLang: ë©”ëª¨ë¦¬ ì œí•œìœ¼ë¡œ ë‹¨ì¼ ëª¨ë¸ë§Œ ì‹¤í–‰ (RTX 5090)")
    print("  - SGLang: RadixAttention ë¹„í™œì„±í™”ë¡œ ì¸í•œ ì„±ëŠ¥ ì œí•œ")

if __name__ == "__main__":
    # ì„œë²„ ì¤€ë¹„ ì‹œê°„
    print("â³ ì„œë²„ ì¤€ë¹„ ì¤‘ (10ì´ˆ ëŒ€ê¸°)...")
    time.sleep(10)

    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    run_benchmark()