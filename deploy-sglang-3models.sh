#!/bin/bash

# SGLang 3ê°œ ëª¨ë¸ ë™ì‹œ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
# RTX 5090 ìµœì í™” ì„¤ì •

echo "ğŸš€ SGLang 3ê°œ ëª¨ë¸ ë°°í¬ ì‹œì‘..."

# ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬
echo "ğŸ“¦ ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬ ì¤‘..."
docker stop sglang-tinyllama sglang-qwen sglang-yi 2>/dev/null
docker rm sglang-tinyllama sglang-qwen sglang-yi 2>/dev/null

echo ""
echo "1ï¸âƒ£ TinyLlama 1.1B ì‹œì‘..."
docker run -d \
  --name sglang-tinyllama \
  --runtime nvidia \
  --gpus all \
  -p 30001:8000 \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -e PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True" \
  -e CUDA_VISIBLE_DEVICES=0 \
  --shm-size 8g \
  sglang:blackwell-final-v2 \
  --model-path TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --host 0.0.0.0 \
  --port 8000 \
  --mem-fraction-static 0.12 \
  --max-total-tokens 2048 \
  --max-running-requests 48 \
  --trust-remote-code \
  --disable-cuda-graph \
  --disable-custom-all-reduce \
  --disable-flashinfer \
  --disable-radix-cache \
  --attention-backend torch_native \
  --dtype float16 \
  --log-level info

echo "â³ TinyLlama ì´ˆê¸°í™” ëŒ€ê¸° (15ì´ˆ)..."
sleep 15

echo ""
echo "2ï¸âƒ£ Qwen 2.5-3B ì‹œì‘..."
docker run -d \
  --name sglang-qwen \
  --runtime nvidia \
  --gpus all \
  -p 30002:8000 \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -e PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True" \
  -e CUDA_VISIBLE_DEVICES=0 \
  --shm-size 16g \
  sglang:blackwell-final-v2 \
  --model-path Qwen/Qwen2.5-3B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --mem-fraction-static 0.20 \
  --max-total-tokens 3072 \
  --max-running-requests 32 \
  --trust-remote-code \
  --disable-cuda-graph \
  --disable-custom-all-reduce \
  --disable-flashinfer \
  --disable-radix-cache \
  --attention-backend torch_native \
  --dtype float16 \
  --log-level info

echo "â³ Qwen ì´ˆê¸°í™” ëŒ€ê¸° (20ì´ˆ)..."
sleep 20

echo ""
echo "3ï¸âƒ£ Yi-6B ì‹œì‘..."
docker run -d \
  --name sglang-yi \
  --runtime nvidia \
  --gpus all \
  -p 30003:8000 \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -e PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True" \
  -e CUDA_VISIBLE_DEVICES=0 \
  --shm-size 24g \
  sglang:blackwell-final-v2 \
  --model-path 01-ai/Yi-6B-Chat \
  --host 0.0.0.0 \
  --port 8000 \
  --mem-fraction-static 0.40 \
  --max-total-tokens 4096 \
  --max-running-requests 24 \
  --trust-remote-code \
  --disable-cuda-graph \
  --disable-custom-all-reduce \
  --disable-flashinfer \
  --disable-radix-cache \
  --attention-backend torch_native \
  --dtype float16 \
  --log-level info

echo "â³ Yi ì´ˆê¸°í™” ëŒ€ê¸° (30ì´ˆ)..."
sleep 30

echo ""
echo "âœ… ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸..."
docker ps | grep sglang

echo ""
echo "ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸..."
nvidia-smi --query-gpu=memory.used,memory.total --format=csv

echo ""
echo "ğŸ” ì„œë²„ ìƒíƒœ í™•ì¸..."
echo "TinyLlama (30001):"
curl -s http://localhost:30001/health || echo "âŒ ì•„ì§ ì¤€ë¹„ ì¤‘..."

echo "Qwen (30002):"
curl -s http://localhost:30002/health || echo "âŒ ì•„ì§ ì¤€ë¹„ ì¤‘..."

echo "Yi (30003):"
curl -s http://localhost:30003/health || echo "âŒ ì•„ì§ ì¤€ë¹„ ì¤‘..."

echo ""
echo "ğŸ¯ ë°°í¬ ì™„ë£Œ!"
echo "ì—”ë“œí¬ì¸íŠ¸:"
echo "  - TinyLlama: http://localhost:30001"
echo "  - Qwen 2.5-3B: http://localhost:30002"
echo "  - Yi-6B: http://localhost:30003"