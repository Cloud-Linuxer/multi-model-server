# LLM Serving Framework Benchmark Report - RTX 5090
# RTX 5090 LLM ì„œë¹™ í”„ë ˆì„ì›Œí¬ ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ

**Date**: 2025-09-18
**Hardware**: NVIDIA RTX 5090 (32GB VRAM)
**Test Model**: TinyLlama 1.1B Chat v1.0

---

## ğŸ“Š Executive Summary / ìš”ì•½

This report presents benchmark results for three LLM serving frameworks tested under various conditions. Due to differences in testing methodologies and measurement approaches, direct comparisons should be interpreted with caution.

ì´ ë³´ê³ ì„œëŠ” ë‹¤ì–‘í•œ ì¡°ê±´ì—ì„œ í…ŒìŠ¤íŠ¸ëœ ì„¸ ê°€ì§€ LLM ì„œë¹™ í”„ë ˆì„ì›Œí¬ì˜ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ë°©ë²•ë¡ ê³¼ ì¸¡ì • ë°©ì‹ì˜ ì°¨ì´ë¡œ ì¸í•´ ì§ì ‘ ë¹„êµ ì‹œ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.

---

## ğŸ”¬ Test Methodology / í…ŒìŠ¤íŠ¸ ë°©ë²•ë¡ 

### Test Configurations / í…ŒìŠ¤íŠ¸ êµ¬ì„±

| Parameter / ë§¤ê°œë³€ìˆ˜ | Configuration / êµ¬ì„± |
|---------------------|---------------------|
| **Model** | TinyLlama 1.1B Chat v1.0 |
| **Prompt Types** | English, Chinese, Korean |
| **Max Tokens** | 100 (requested) |
| **Temperature** | 0.7 |
| **Iterations** | 100-300 per language |
| **Deployment** | Docker containers |

### Important Methodological Differences / ì¤‘ìš”í•œ ë°©ë²•ë¡ ì  ì°¨ì´

1. **Token Counting Methods / í† í° ê³„ì‚° ë°©ì‹**:
   - Some tests counted actual generated words (word-based)
   - Others counted tokenizer tokens (subword-based)
   - This creates significant variance in throughput calculations

2. **Test Conditions / í…ŒìŠ¤íŠ¸ ì¡°ê±´**:
   - Different tests used different prompt lengths
   - Warm vs cold start not consistently controlled
   - Network overhead varied between tests

---

## ğŸ“ˆ Benchmark Results / ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼

### Raw Performance Metrics / ì›ì‹œ ì„±ëŠ¥ ì§€í‘œ

| Framework | Test Type | Avg Latency (ms) | Throughput* | Sample Size | Success Rate |
|-----------|-----------|------------------|-------------|-------------|--------------|
| **vLLM** | Unified Test | 207.1 | 13.3 tok/sâ€  | 299 | 99.7% |
| **SGLang** | Dedicated Test | 356.8 | 76.8 tok/s | 150 | 100% |
| **Ollama** | Unified Test | 151.8 | 64.2 tok/sâ€  | 300 | 100% |
| **Ollama** | Dedicated Test | 347.7 | 127.6 tok/s | 73 | 100% |

*Throughput measurements are not directly comparable due to different token counting methods
*ì²˜ë¦¬ëŸ‰ ì¸¡ì •ê°’ì€ í† í° ê³„ì‚° ë°©ì‹ì˜ ì°¨ì´ë¡œ ì§ì ‘ ë¹„êµ ë¶ˆê°€
â€ Word-based counting (significantly lower than tokenizer-based)

### Response Time Distribution / ì‘ë‹µ ì‹œê°„ ë¶„í¬

| Framework | P50 (ms) | P95 (ms) | Min (ms) | Max (ms) |
|-----------|----------|----------|----------|----------|
| **vLLM** | 257 | 261 | 10 | 14,404* |
| **SGLang** | ~350 | ~540 | ~180 | ~550 |
| **Ollama** | 150 | 152 | ~140 | ~160 |

*Includes timeout and cold start outliers

---

## ğŸ’¾ Memory Usage Analysis / ë©”ëª¨ë¦¬ ì‚¬ìš© ë¶„ì„

| Framework | Single Model | Multi-Model Support | Memory Efficiency |
|-----------|--------------|---------------------|-------------------|
| **vLLM** | ~9GB | Yes (3 models: 27GB) | Static allocation |
| **SGLang** | ~5GB | Limitedâ€  | Lower baseline |
| **Ollama** | ~3GB | Yes (dynamic: 8.5GB) | Dynamic loading |

â€ Multi-model support encountered issues on RTX 5090

---

## âš–ï¸ Comparative Analysis / ë¹„êµ ë¶„ì„

### Strengths and Limitations / ê°•ì ê³¼ í•œê³„

#### vLLM
**Strengths / ê°•ì **:
- Mature multi-model support / ì„±ìˆ™í•œ ë©€í‹°ëª¨ë¸ ì§€ì›
- Production-ready features / í”„ë¡œë•ì…˜ ì¤€ë¹„ ê¸°ëŠ¥
- OpenAI API compatibility / OpenAI API í˜¸í™˜ì„±

**Limitations / í•œê³„**:
- Higher memory usage / ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- Variable performance in tests / í…ŒìŠ¤íŠ¸ì—ì„œ ê°€ë³€ì  ì„±ëŠ¥

#### SGLang
**Strengths / ê°•ì **:
- Lower memory baseline / ë‚®ì€ ë©”ëª¨ë¦¬ ê¸°ì¤€ì„ 
- Specialized optimization potential / íŠ¹í™”ëœ ìµœì í™” ê°€ëŠ¥ì„±

**Limitations / í•œê³„**:
- Multi-model challenges on RTX 5090 / RTX 5090ì—ì„œ ë©€í‹°ëª¨ë¸ ë¬¸ì œ
- Docker deployment complexities / Docker ë°°í¬ ë³µì¡ì„±

#### Ollama
**Strengths / ê°•ì **:
- Memory efficient dynamic loading / ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë™ì  ë¡œë”©
- Consistent low latency in unified tests / í†µí•© í…ŒìŠ¤íŠ¸ì—ì„œ ì¼ê´€ëœ ë‚®ì€ ì§€ì—°ì‹œê°„
- Simple deployment / ê°„ë‹¨í•œ ë°°í¬

**Limitations / í•œê³„**:
- Performance varies by test type / í…ŒìŠ¤íŠ¸ ìœ í˜•ë³„ ì„±ëŠ¥ ì°¨ì´
- Less feature-rich than vLLM / vLLMë³´ë‹¤ ê¸°ëŠ¥ ë¶€ì¡±

---

## ğŸ“‹ Use Case Recommendations / ì‚¬ìš© ì‚¬ë¡€ ê¶Œì¥ì‚¬í•­

| Use Case / ì‚¬ìš© ì‚¬ë¡€ | Recommended Framework | Rationale / ê·¼ê±° |
|---------------------|----------------------|------------------|
| **Production API Services** | vLLM or Ollama | Depends on feature vs efficiency needs |
| **Memory-Constrained Environments** | Ollama | Dynamic loading reduces memory footprint |
| **Multi-Model Serving** | vLLM | Most mature multi-model support |
| **Development/Testing** | Ollama | Simple setup and management |
| **Single Model Optimization** | Any | All perform adequately for single model |

---

## âš ï¸ Important Considerations / ì¤‘ìš” ê³ ë ¤ì‚¬í•­

1. **Measurement Inconsistencies / ì¸¡ì • ë¶ˆì¼ì¹˜**:
   - Token counting methods significantly affect throughput metrics
   - Direct throughput comparisons may be misleading
   - Latency measurements are more reliable for comparison

2. **Test Environment Factors / í…ŒìŠ¤íŠ¸ í™˜ê²½ ìš”ì¸**:
   - Docker overhead affects all frameworks
   - RTX 5090 compatibility varies
   - Network latency impacts API-based tests

3. **Statistical Validity / í†µê³„ì  ìœ íš¨ì„±**:
   - Sample sizes vary (73-300 requests)
   - Not all tests covered all scenarios
   - Results may not generalize to other hardware

---

## ğŸ” Methodology Transparency / ë°©ë²•ë¡  íˆ¬ëª…ì„±

### Data Sources / ë°ì´í„° ì¶œì²˜
- `unified_benchmark_20250918_215507.csv`: Comparative test
- `benchmark_sglang_20250918_153246.csv`: SGLang dedicated test
- `ollama_benchmark_20250918_211500.csv`: Ollama dedicated test
- `vllm_benchmark_20250918_155524.csv`: vLLM dedicated test

### Limitations / í•œê³„
1. Tests conducted at different times with potentially different system loads
2. Token counting methodology not standardized across tests
3. Limited to single model (TinyLlama 1.1B) performance
4. RTX 5090 specific - results may not generalize to other GPUs

---

## ğŸ“ Conclusion / ê²°ë¡ 

The benchmark results show that each framework has distinct characteristics:

- **Ollama** demonstrated lower latency in unified tests but results varied significantly between test types
- **vLLM** showed stable multi-model support despite variable throughput measurements
- **SGLang** faced deployment challenges that limited comprehensive testing

The significant variance in token counting methods makes throughput comparisons unreliable. **Latency measurements provide more consistent comparison basis**. Framework selection should be based on specific use case requirements rather than raw performance metrics alone.

ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ëŠ” ê° í”„ë ˆì„ì›Œí¬ê°€ ëšœë ·í•œ íŠ¹ì„±ì„ ê°€ì§€ê³  ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. í† í° ê³„ì‚° ë°©ë²•ì˜ ì°¨ì´ë¡œ ì¸í•´ ì²˜ë¦¬ëŸ‰ ë¹„êµëŠ” ì‹ ë¢°í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. **ì§€ì—°ì‹œê°„ ì¸¡ì •ì´ ë” ì¼ê´€ëœ ë¹„êµ ê¸°ì¤€ì„ ì œê³µí•©ë‹ˆë‹¤**. í”„ë ˆì„ì›Œí¬ ì„ íƒì€ ë‹¨ìˆœí•œ ì„±ëŠ¥ ì§€í‘œë³´ë‹¤ëŠ” íŠ¹ì • ì‚¬ìš© ì‚¬ë¡€ ìš”êµ¬ì‚¬í•­ì„ ê¸°ë°˜ìœ¼ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤.

---

*This report aims to present benchmark data objectively. Users should conduct their own testing for their specific use cases.*
*ì´ ë³´ê³ ì„œëŠ” ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ë¥¼ ê°ê´€ì ìœ¼ë¡œ ì œì‹œí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ì‚¬ìš©ìëŠ” íŠ¹ì • ì‚¬ìš© ì‚¬ë¡€ì— ëŒ€í•´ ìì²´ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.*